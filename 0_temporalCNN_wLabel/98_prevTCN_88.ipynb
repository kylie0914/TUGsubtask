{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from itertools import combinations\n",
    "import tensorflow as tf\n",
    "#import csv # to read string (color_*.jpg)\n",
    "#from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(7)\n",
    "np.set_printoptions(precision=4, suppress=True)  # 과학적 표기 대신 소숫점 자리 4자리까지 표현\n",
    "\n",
    "rootDir = \"D:/바탕화면/Dataset/TUG/trainSet\"\n",
    "expertFolder = \"/\" + \"2021_01_24_saveResults_최윤정\"  # --- 변경 할 부분\n",
    "viewFolder = \"/\" + \"0_sideView\"\n",
    "\n",
    "print(\" [Home Dir.] Current path\", os.getcwd())  \n",
    "\n",
    "datasetDir = rootDir + expertFolder + viewFolder\n",
    "os.chdir(datasetDir) ; print(\" Current workaing path [dataset] \", os.getcwd())  # -- Dataset 있는 곳으로 경로 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [before shuffle] 1D sub list:  ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'lhs', 'NHJ_60', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj', 'cyj2', 'jdh', 'jek', 'kch', 'ryu', 'JDW_tug', 'BYS_TUG', 'CYJ_TUG', 'PHE_TUG', 'YSJ_TUG']\n",
      "\t--> # of train sub: 18명  ,  # of test subjects: 7명\n"
     ]
    }
   ],
   "source": [
    "def trainData_split(datasetDir,train_ratio = 0.7, shuffle = False,  train_Kfold = False):      \n",
    "   # print(next(os.walk(datasetDir))[1]) #['2020_11_03', '2020_11_20_v1', '2020_11_20_v2', '2020_11_23', '2020_12_09', '2020_12_30']\n",
    "    expDates = next(os.walk(datasetDir))[1]  \n",
    "    \n",
    "    subject_number = 0\n",
    "    subjects_list = []\n",
    "    \n",
    "    for dateFolder in expDates:\n",
    "        dateDir = os.path.join(datasetDir, dateFolder)   # D:/바탕화면/Dataset/TUG/trainSet/2021_01_24_saveResults_최윤정/0_sideView\\2020_11_03\n",
    "        tmpSubjects = next(os.walk(dateDir))[1]\n",
    "        subjects_list.append(tmpSubjects)  \n",
    "        subject_number += len(tmpSubjects)\n",
    "#     print(subjects_list)      # [['bys_tug', 'cbd_tug', 'cyj_tug'], ['kw', 'kyh', 'lhs', 'NHJ_60', 'nhs', 'pjh', 'pss', 'rjh', 'yjh']]\n",
    "#     print(subject_number)     25\n",
    "\n",
    "    # ------ 2D -> 1D [[sub1, sub2], [sub3, sub4]] -> [sub1, sub2, sub3, sub4]\n",
    "    subjects = []\n",
    "    for eachSub in subjects_list:\n",
    "        subjects += eachSub\n",
    "    print(\" [before shuffle] 1D sub list: \" , subjects)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(subjects)\n",
    "        print(\" [After shuffle] 1D sub list: \" , subjects)\n",
    "        \n",
    "    # ---- Split train/test subjects\n",
    "    trainSub_number = np.round( (len(subjects)*train_ratio), 0).astype(int)\n",
    "    testSub_number = subject_number - trainSub_number\n",
    "    print(\"\\t--> # of train sub: \" + str(trainSub_number) +\"명  ,  # of test subjects: \" + str(testSub_number) + \"명\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_Fold = list(combinations(subjects, trainSub_number))  # 25개 중 18개 뽑는 경우의 수 ==  25! / ((25 - 18)! * 18!) = 480700 개의 조합                     \n",
    "    if train_Kfold:\n",
    "        print(\" [K-fold] combinations( \", subject_number, \",\", trainSub_number, \" )\")\n",
    "        return train_Fold\n",
    "    else:\n",
    "        return train_Fold[0]\n",
    "\n",
    "train_subjects = trainData_split(datasetDir,train_ratio = 0.7, shuffle = False, train_Kfold = False)  \n",
    "\n",
    "# # #     임의로 train, test set 만들기\n",
    "# train_subjects =  ['NHJ_60',  'yjh', 'kyh', 'pss' ,'cyj_tug', 'cbd', 'cyj2', 'bys', 'jdh', 'jek', 'kch', 'kw', 'nhs', 'rjh'] \n",
    "# normal_subjects = ['lhs', 'pjh', 'cbd_tug', 'bys_tug','cyj','JDW_tug_fixed']\n",
    "# patients_subjects = ['JDW_tug_fixed']\n",
    "# test_subjects = patients_subjects # patients_subjects # normal_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Shape X , Y: (69605, 3) , (69605, 5) \n",
      " Test Shape X , Y: (36119, 3) , (36119, 5)\n"
     ]
    }
   ],
   "source": [
    "numFeature = 3\n",
    "numActions = 5\n",
    "\n",
    "def getPelvisData(csvFile):\n",
    "    rawData = np.loadtxt(csvFile, delimiter=\",\")\n",
    "    timestamp = rawData[:,0]\n",
    "    pelvis_x = rawData[:,1]\n",
    "    pelvis_y = rawData[:,2]\n",
    "    pelvis_z = rawData[:,3]\n",
    "    actionList = rawData[:,4:9]\n",
    "    return timestamp, pelvis_x, pelvis_y, pelvis_z, actionList\n",
    "\n",
    "def create_dataset(train_subjects):\n",
    "    train_x = np.zeros((0, numFeature))\n",
    "    train_y = np.zeros((0, numActions))\n",
    "    test_x = np.zeros((0, numFeature))\n",
    "    test_y = np.zeros((0, numActions))    \n",
    "    \n",
    "    trainSet = None\n",
    "    testSet = None\n",
    "    \n",
    "    for dirpath, foldername, files in os.walk(datasetDir):\n",
    "        for filename in files:\n",
    "            if \"lpf_\" in filename:\n",
    "                subname = dirpath.split(\"\\\\\")[2] \n",
    "                csvFile = os.path.join(dirpath, filename)\n",
    "                timestamp, pelvis_x, pelvis_y, pelvis_z, actionList = getPelvisData(csvFile) \n",
    "                \n",
    "                pelvisData = np.array([pelvis_x, pelvis_y, pelvis_z]).T\n",
    "                actionData = np.array(actionList)\n",
    "                dataset = tf.data.Dataset.from_tensor_slices( (pelvisData.astype('float32'), actionData.astype('float32')) ) \n",
    "                \n",
    "                \n",
    "                if subname in train_subjects:\n",
    "                    train_x = np.append(train_x, pelvisData, axis = 0 )\n",
    "                    train_y = np.append(train_y, actionData, axis = 0 )\n",
    "\n",
    "                    if trainSet == None:\n",
    "                        trainSet = dataset\n",
    "                    else:\n",
    "                        trainSet = trainSet.concatenate(dataset)  \n",
    "                else:\n",
    "#                 if subname in test_subjects:\n",
    "                    test_x = np.append(test_x, pelvisData, axis = 0 )\n",
    "                    test_y = np.append(test_y, actionData, axis = 0 )    \n",
    "\n",
    "                    if testSet == None:\n",
    "                        testSet = dataset\n",
    "                    else:\n",
    "                        testSet = testSet.concatenate(dataset)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "         \n",
    "train_x, train_y, test_x, test_y = create_dataset(train_subjects)\n",
    "print(\" Train Shape X , Y: {0} , {1} \\n Test Shape X , Y: {2} , {3}\".format(train_x.shape, train_y.shape, test_x.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_to_section(data_x, data_y, numActions, sliding_window_size, step_size_of_sliding_window, standardize=False, **options):\n",
    "    data = data_x  # sensor data\n",
    "    act_labels = data_y  # action labels\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    \n",
    "    trainSet = None\n",
    "    testSet = None\n",
    "    if standardize:\n",
    "            # As usual, normalize test dataset by training dataset's parameters\n",
    "        if options:\n",
    "            mean = options.get(\"mean\")\n",
    "            std = options.get(\"std\")\n",
    "                # \\n test mean: {0}, std: {1}\\n\\n\".format(mean,std))\n",
    "            print(\"Test Data has been standardized:\")\n",
    "        else:\n",
    "                # csv 한 줄의 mean ..이걸 모든 sensor data들에 대해 수행\n",
    "            mean = data.mean(axis=0)\n",
    "            std = data.std(axis=0)\n",
    "            print(\"Training Data has been standardized:\\n the mean is = \", str(mean.mean()), \" ; and the std is = \", str(std.mean()))\n",
    "            # mean removal and variance scaling\n",
    "        data -= mean\n",
    "        data /= std\n",
    "    else:\n",
    "        print(\"----> Without Standardization.....\")\n",
    "\n",
    "        # We want the Rows of matrices show each Feature and the Columns show time points.\n",
    "        # before transepose,,(145687, 12) -->  After (12, 145687)\n",
    "    data = data.T\n",
    "\n",
    "    size_features = data.shape[0]  # 4\n",
    "    size_data = data.shape[1]\n",
    "\n",
    "    number_of_secs = round( ((size_data - sliding_window_size)/step_size_of_sliding_window) )\n",
    "\n",
    "        # Create a 3D matrix for Storing Snapshots\n",
    "    secs_data = np.zeros((number_of_secs, size_features, sliding_window_size))\n",
    "    act_secs_labels = np.zeros((number_of_secs, numActions))\n",
    "    print(\"number of sections: \", number_of_secs, \"size of data\", size_data )\n",
    "    k = 0\n",
    "    for i in range(0, (size_data) - sliding_window_size, step_size_of_sliding_window):\n",
    "        j = i // step_size_of_sliding_window # 0, 3, 6, 12\n",
    "        if(j >= number_of_secs):\n",
    "            break\n",
    "\n",
    "        if(not (act_labels[i] == act_labels[i+sliding_window_size-1]).all()):\n",
    "            continue\n",
    "\n",
    "        secs_data[k] = data[0:size_features, i:i+sliding_window_size]\n",
    "        act_secs_labels[k] = act_labels[i].astype(int)\n",
    "        k = k+1\n",
    "    secs_data = secs_data[0:k]    \n",
    "    act_secs_labels = act_secs_labels[0:k]\n",
    "\n",
    "    return secs_data, act_secs_labels, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Sectioning Training and Test datasets: shape of each section will be: ( 3 x 15 )\n",
      "Training Data has been standardized:\n",
      " the mean is =  0.4438291443250706  ; and the std is =  0.26557480458427785\n",
      "number of sections:  23197 size of data 69605\n",
      "Test Data has been standardized:\n",
      "number of sections:  12035 size of data 36119\n",
      "\n",
      "\n",
      "---> Training X Section: (18206, 3, 15) , Training Label Section: (18206, 5) \n",
      "---> Test X Sections: (10753, 3, 15) , Test Label Sections: (10753, 5)\n"
     ]
    }
   ],
   "source": [
    "sliding_window_size = 15 # 30.. 1sec.. 50 Equals to 1 sec for MotionSense Dataset (it is on 50Hz samplig rate)\n",
    "step_size_of_sliding_window = 3 #10 \n",
    "print(\"--> Sectioning Training and Test datasets: shape of each section will be: (\", numFeature, \"x\", sliding_window_size, \")\")\n",
    "\n",
    "train_x_secs, train_y_secs, train_mean, train_std = time_series_to_section(train_x.copy(), train_y.copy(), numActions, sliding_window_size,step_size_of_sliding_window, standardize = True)\n",
    "\n",
    "test_x_secs, test_y_secs, test_mean, test_std = time_series_to_section(test_x.copy(), test_y.copy(), numActions, sliding_window_size, step_size_of_sliding_window, standardize = True, mean = train_mean, std = train_std)\n",
    "\n",
    "print(\"\\n\\n---> Training X Section: {0} , Training Label Section: {1} \\n---> Test X Sections: {2} , Test Label Sections: {3}\".format(train_x_secs.shape, train_y_secs.shape, test_x_secs.shape, test_y_secs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Training Section: (18206, 3, 15, 1) , Test Sections: (10753, 3, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D , MaxPooling2D, Dropout\n",
    "\n",
    "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
    "train_x_block = np.expand_dims(train_x_secs, axis=3)\n",
    "test_x_block = np.expand_dims(test_x_secs, axis=3)\n",
    "print(\"---> Training Section: {0} , Test Sections: {1}\".format(train_x_block.shape, test_x_block.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, height, width, channel = train_x_block.shape  #   (13298, 4, 15, 1) =  (number_of_secs , size_features , sliding_window_size, 1)\n",
    "metrics = ['acc']\n",
    "\n",
    "## MTCNN\n",
    "drop_prob_1 = 0.2 \n",
    "drop_prob_2 = 0.4 \n",
    "\n",
    "\n",
    "## Action model \n",
    "action_layer_dim = numActions # 4  # dws, ups, wlk, jog # , sit,  std \n",
    "action_activation_func = 'softmax'\n",
    "action_loss_func = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "def activityModel():\n",
    "    input_layer = Input(shape = (height, width, 1) ) # (4, 15, 1) = (feature, sliding_window, 1)\n",
    "\n",
    "    conv_0 = Conv2D(15, kernel_size=(1, 5), strides=(1, 1), padding='valid',  activation='relu') (input_layer) #5\n",
    "    conv_1 = Conv2D(15, kernel_size=(1, 5), strides=(1, 1), padding='same', activation='relu') (conv_0)\n",
    "    dense_1 = Dense(50, activation='relu') (conv_1)\n",
    "    pool_1 = MaxPooling2D(pool_size=(1,2)) (dense_1)\n",
    "    drop_1 = Dropout(drop_prob_1)(pool_1)\n",
    "\n",
    "\n",
    "    conv_2 = Conv2D(11, kernel_size=(1, 3), strides=(1, 1), padding='valid',  activation='relu') (drop_1)# 5\n",
    "    dense_2 = Dense(40, activation='relu') (conv_2)\n",
    "    pool_2 = MaxPooling2D(pool_size=(1, 3)) (dense_2)  \n",
    "    drop_2 = Dropout(drop_prob_1) (pool_2)\n",
    "\n",
    "    conv_3 = Conv2D(10, kernel_size=(1, 3), strides=(1, 1), padding='same',  activation='relu') (drop_2)\n",
    "    drop_3 = Dropout(drop_prob_1)(conv_3)\n",
    "\n",
    "    flat = Flatten() (drop_3)\n",
    "    hidden = Dense(400, activation = 'relu') (flat)\n",
    "    drop_4 = Dropout(drop_prob_2)(hidden)\n",
    "\n",
    "    action_out = Dense(action_layer_dim, activation = action_activation_func, name = 'ACTION') (drop_4)\n",
    "    \n",
    "    # --------- Model input/ Output Definition\n",
    "    activity_model = Model(inputs=input_layer, outputs = action_out)\n",
    "    activity_model.summary()\n",
    "\n",
    "    activity_model.compile(loss=action_loss_func, optimizer=keras.optimizers.Adam(0.001), metrics=metrics)\n",
    "\n",
    "    return activity_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3, 15, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 3, 11, 15)         90        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 11, 15)         1140      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3, 11, 50)         800       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 5, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 11)          1661      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3, 3, 40)          480       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 3, 1, 40)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 1, 40)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 1, 10)          1210      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 1, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               12400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "ACTION (Dense)               (None, 5)                 2005      \n",
      "=================================================================\n",
      "Total params: 19,786\n",
      "Trainable params: 19,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.5780 - acc: 0.7789\n",
      "Epoch 2/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.1621 - acc: 0.9500\n",
      "Epoch 3/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0623 - acc: 0.9829\n",
      "Epoch 4/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0382 - acc: 0.9899\n",
      "Epoch 5/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0448 - acc: 0.9889\n",
      "Epoch 6/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0340 - acc: 0.9915\n",
      "Epoch 7/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0312 - acc: 0.9919\n",
      "Epoch 8/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0264 - acc: 0.9929\n",
      "Epoch 9/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0281 - acc: 0.9924\n",
      "Epoch 10/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0264 - acc: 0.9930\n",
      "Epoch 11/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0277 - acc: 0.9918\n",
      "Epoch 12/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0256 - acc: 0.9931\n",
      "Epoch 13/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0258 - acc: 0.9932\n",
      "Epoch 14/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0200 - acc: 0.9942\n",
      "Epoch 15/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0231 - acc: 0.9943\n",
      "Epoch 16/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0236 - acc: 0.9938\n",
      "Epoch 17/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0221 - acc: 0.9941\n",
      "Epoch 18/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0205 - acc: 0.9945\n",
      "Epoch 19/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0225 - acc: 0.9941\n",
      "Epoch 20/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0218 - acc: 0.9930\n",
      "Epoch 21/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0214 - acc: 0.9943\n",
      "Epoch 22/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0182 - acc: 0.9945\n",
      "Epoch 23/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0188 - acc: 0.9946\n",
      "Epoch 24/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0242 - acc: 0.9932\n",
      "Epoch 25/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 26/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0190 - acc: 0.9943\n",
      "Epoch 27/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0188 - acc: 0.9956\n",
      "Epoch 28/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0167 - acc: 0.9953\n",
      "Epoch 29/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0173 - acc: 0.9952\n",
      "Epoch 30/30\n",
      "285/285 [==============================] - 1s 3ms/step - loss: 0.0161 - acc: 0.9953\n"
     ]
    }
   ],
   "source": [
    "## Training Phase\n",
    "batch_size = 64\n",
    "num_of_epochs = 30 # 30\n",
    "verbosity = 1\n",
    "\n",
    "# Model Training \n",
    "model = activityModel()\n",
    "history = model.fit(train_x_block, train_y_secs,                \n",
    "              batch_size = batch_size,\n",
    "              epochs = num_of_epochs,\n",
    "              verbose = verbosity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 0s 650us/step - loss: 0.8603 - acc: 0.8600\n",
      "--> Evaluation on Test Dataset:\n",
      "**** Accuracy for Activity Recognition task is:  [0.8603415489196777, 0.8600390553474426]\n",
      " correct:  0.8600390588672928  [%] ,  incorrect:  0.13996094113270716  [%]\n"
     ]
    }
   ],
   "source": [
    "#------------- Evaluation \n",
    "results_1 = model.evaluate(test_x_block, test_y_secs, verbose = verbosity)\n",
    "print(\"--> Evaluation on Test Dataset:\")\n",
    "print(\"**** Accuracy for Activity Recognition task is: \", results_1)\n",
    "\n",
    "\n",
    "# ----------------- \n",
    "y_predict = model.predict(test_x_block) # (6161, 5)  \n",
    "y_pred_index = y_predict.argmax(axis=1) # print(y_pred_index) \n",
    "y_true_idx = np.argmax(test_y_secs, axis=1) # print(y_true_idx)\n",
    "\n",
    "correct = np.nonzero(y_true_idx == y_pred_index) # print(correct, len(correct[0]))\n",
    "incorrect = np.nonzero(y_true_idx != y_pred_index) # print(incorrect, len(incorrect[0]))\n",
    "\n",
    "valid_num = len(correct[0]) + len(incorrect[0])\n",
    "tot_num = len(y_true_idx)\n",
    "# print(\"[must same] # of real data: \", tot_num, \"==  # of pred data: \", valid_num)\n",
    "percent_correct = len(correct[0]) /tot_num\n",
    "percent_incorrect = len(incorrect[0])/tot_num\n",
    "print(\" correct: \", percent_correct,\" [%]\", \",  incorrect: \", percent_incorrect,\" [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = test_x.copy()\n",
    "tYact = test_y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(y_pred_index)\n",
    "print(y_predict)\n",
    "\n",
    "\n",
    "# # print(y_predict[y_pred_index])\n",
    "# raw_gyro = y_predict[np.argmax(y_pred_index)]\n",
    "# plt.plot( y_pred_index, \"b--\", linewidth=0.5, markersize=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polish-acrylic",
   "metadata": {},
   "source": [
    "### Get Pelvis data - LPF + Norm (save Figure + csv)\n",
    "### Get oneHot encoded Label \n",
    "### Interpolation (Univariate Interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bronze-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Home Dir.] Current path /DockerProjects/walkCAM/tug/temporalCNN_wLabel\n",
      " Current workaing path [dataset]  /DockerProjects/Dataset/TUG/trainSet/HMMpaper/LSTM2HMM_saveResults_illness_KYH/0_sideView\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold  # 라벨 x_data, y_data 분포를 유지 (함께 fold)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "homeDir = \"/DockerProjects/walkCAM/tug/temporalCNN_wLabel\"\n",
    "os.chdir(homeDir); print(\" [Home Dir.] Current path\", os.getcwd())  \n",
    "\n",
    "\n",
    "from TCN_v1_Lib.preprocessing import preprocess\n",
    "from TCN_v1_Lib.plot_graph import plotGragh\n",
    "\n",
    "np.random.seed(7)\n",
    "np.set_printoptions(precision=4, suppress=True)  # 과학적 표기 대신 소숫점 자리 4자리까지 표현\n",
    "\n",
    "\n",
    "rootDir = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper\" \n",
    "# expertFolder = \"/\" + \"HMM_saveResults_illness_CYJ\"  # --- 변경 할 부분\n",
    "# expertFolder = \"/\" + \"LSTM2HMM_saveResults_illness_KHJ\"  # --- 변경 할 부분  stroke 이상함 \n",
    "expertFolder = \"/\" + \"LSTM2HMM_saveResults_illness_KYH\"  # --- 변경 할 부분\n",
    "viewFolder = \"/\" + \"0_sideView\"\n",
    "\n",
    "datasetDir = rootDir + expertFolder + viewFolder\n",
    "os.chdir(datasetDir) ; print(\" Current workaing path [dataset] \", os.getcwd())  # -- Dataset 있는 곳으로 경로 변경 \n",
    "\n",
    "\n",
    "utils = preprocess(datasetDir, expertFolder)\n",
    "plot = plotGragh(actSplit=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-concert",
   "metadata": {},
   "source": [
    "### Train/Test Subject Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valid-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_trainNames(subList, Kfold_num = 5):\n",
    "    assert (Kfold_num is not 0), print(\"check kfold_num. should not be zero\")\n",
    "    Fold_results =[]\n",
    "    kfold = KFold(n_splits=  Kfold_num)\n",
    "    \n",
    "    for trainIdx, testIdx in kfold.split(subList):\n",
    "        Fold_results.append([trainIdx, testIdx])      \n",
    "        \n",
    "    train_Fold= []\n",
    "    test_Fold = []\n",
    "        \n",
    "    for i in range(Kfold_num):    # bind fold pair\n",
    "        tmp_train = []\n",
    "        tmp_test = []\n",
    "        for subIdx in range(len(subList)): \n",
    "            if subIdx in Fold_results[i][0]:  # -- K-fold train\n",
    "                tmp_train.append(subList[subIdx])\n",
    "               \n",
    "            if subIdx in Fold_results[i][1]:  # -- K-fold test\n",
    "                tmp_test.append(subList[subIdx])\n",
    "          \n",
    "        train_Fold.append(tmp_train)\n",
    "        test_Fold.append(tmp_test)\n",
    "        print(\"\\t [process]\", i, \"-fold:\", train_Fold[i], \"\\n\\t\\t\\t   ,\", test_Fold[i])\n",
    "    print(\"\\n [results]  trainSet: \", len(train_Fold[0]) , \" 명 , testSet: \", len(test_Fold[0]), \" 명\" )\n",
    "    \n",
    "    return train_Fold, test_Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-station",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Group Type: dict_keys(['norm20', 'norm60', 'stroke'])\n",
      " --> selected Group: norm20 \t, # of Subjects: 17\n",
      " K-fold :  5\n",
      "\t [process] 0 -fold: ['pjh', 'pss', 'yjh', 'kyh', 'lhs', 'nhs', 'bys', 'cyj', 'cbd', 'cyj2', 'jdh', 'jet', 'kch'] \n",
      "\t\t\t   , ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw']\n",
      "\t [process] 1 -fold: ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw', 'lhs', 'nhs', 'bys', 'cyj', 'cbd', 'cyj2', 'jdh', 'jet', 'kch'] \n",
      "\t\t\t   , ['pjh', 'pss', 'yjh', 'kyh']\n",
      "\t [process] 2 -fold: ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw', 'pjh', 'pss', 'yjh', 'kyh', 'cyj', 'cbd', 'cyj2', 'jdh', 'jet', 'kch'] \n",
      "\t\t\t   , ['lhs', 'nhs', 'bys']\n",
      "\t [process] 3 -fold: ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw', 'pjh', 'pss', 'yjh', 'kyh', 'lhs', 'nhs', 'bys', 'jdh', 'jet', 'kch'] \n",
      "\t\t\t   , ['cyj', 'cbd', 'cyj2']\n",
      "\t [process] 4 -fold: ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw', 'pjh', 'pss', 'yjh', 'kyh', 'lhs', 'nhs', 'bys', 'cyj', 'cbd', 'cyj2'] \n",
      "\t\t\t   , ['jdh', 'jet', 'kch']\n",
      "\n",
      " [results]  trainSet:  13  명 , testSet:  4  명\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.6\n",
    "valid_ratio = 0.2\n",
    "\n",
    "shuffle = True\n",
    "arbitrarySplit = False\n",
    "Kfold = True\n",
    "Kfold_num = 5 \n",
    "\n",
    "norm20 = ['bys_tug', 'cyj_tug', 'cbd_tug', 'kw' , 'pjh',     'pss', 'yjh', 'kyh' , 'lhs', 'nhs',    'bys', 'cyj', 'cbd', 'cyj2', 'jdh',   'jet', 'kch']\n",
    "norm60 = ['rjh', 'NHJ_60', 'LKO_TUG', 'LSJ_TUG',    'GYH_TUG', 'KYS3_TUG','PBK_TUG','KJK_TUG', 'UMS_TUG','JJS_TUG',   'HOJ_TUG','SJJ_TUG','YSS2_TUG',   'KJH_TUG','CSY_TUG','LBL_TUG','BKS_TUG', \n",
    "            'LSC_TUG','YMR_TUG','PSM_TUG','HSN_TUG','SOS_TUG',     'LSS_TUG', 'KHM_TUG', 'LJS_TUG', 'LJE_TUG',     'KBH_TUG' ]\n",
    "stroke = ['KNG_TUG', 'KSG_TUG', 'YBJ_TUG', 'RJD_TUG',  'SRK_TUG',     'KYS_TUG', 'KTS_TUG','SRH_TUG', 'HHS_TUG', 'KYS2_TUG',      'KSJ_TUG', 'KYB_TUG', 'CES_TUG', 'PCW_TUG', 'WJS_TUG',\n",
    "            'HKH_TUG', 'HJY_TUG', 'SHW_TUG']   \n",
    "\n",
    "selected_type = \"norm20\"\n",
    "\n",
    "group = {\"norm20\": norm20 , \"norm60\": norm60, \"stroke\": stroke} # -------------------------------------------------- Change This part \n",
    "numOfsubjects = len( group[selected_type] )\n",
    "\n",
    "trainFold = None\n",
    "testFold = None\n",
    "\n",
    "specific_group =  group.get(selected_type)\n",
    "print(\" Group Type: {0}\\n --> selected Group: {1} \\t, # of Subjects: {2}\".format(group.keys(), selected_type, numOfsubjects))\n",
    "\n",
    "if Kfold:\n",
    "\n",
    "    print(\" K-fold : \", Kfold_num)\n",
    "    trainFold, testFold = fold_trainNames(subList = specific_group, Kfold_num = Kfold_num)\n",
    "        \n",
    "else:\n",
    "    np.random.shuffle(specific_group)\n",
    "    \n",
    "    trainSub_number = np.round( (numOfsubjects*train_ratio), 0).astype(int)\n",
    "    validSub_number = np.round( (numOfsubjects*valid_ratio), 0).astype(int)\n",
    "    testSub_number = numOfsubjects - trainSub_number - validSub_number\n",
    "\n",
    "    train_subjects = specific_group[ :trainSub_number]\n",
    "    valid_subjects = specific_group[ trainSub_number : (trainSub_number + validSub_number)]\n",
    "    test_subjects = specific_group[ (trainSub_number+validSub_number): ]\n",
    "    \n",
    "    print(\" No-fold --- (arbitrary) \\t [results] \\n\\t Train 명: \" ,len(train_subjects), \"\\n\\t Valid 명: \", len(valid_subjects), \"\\n\\t Test 명: \", len(test_subjects))\n",
    "    print(\" No-fold --- (arbitrary) \\t [results] \\n\\t Train: \" ,train_subjects, \"\\n\\t Valid: \", valid_subjects, \"\\n\\t Test: \", test_subjects)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-safety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "level-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Activation, Flatten, Dense, Conv2D\n",
    "from keras.layers import BatchNormalization, add , Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tcn import TCN\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #Residual block :: https://roadcom.tistory.com/95\n",
    "def ResBlock(x,filters,kernel_size,dilation_rate):\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate,activation='relu')(x) #first convolution\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate)(r) #Second convolution\n",
    "    if x.shape[-1]==filters:\n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 일치할 경우 단순 add 연산만 진행하는 블록 = identity block\n",
    "        shortcut = x  # identity block \n",
    "    else: \n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 다를 경우 shortcut path 를 적절히 변환\n",
    "        # 즉, projection 을 통해 channel 을 맞춰주는 작업이(projection shortcut) 추가되기에 이를 convolution block 이라함\n",
    "        shortcut=Conv2D(filters,kernel_size=kernel_size,padding='same')(x) \n",
    "    o=add([r,shortcut])\n",
    "    o=Activation('relu')(o) \n",
    "    return o\n",
    " \n",
    " #Sequence Model\n",
    "def TCN(optimizer='adam'):\n",
    "    kernel_size = (3,3)\n",
    "    input_shape =  (utils.lookback_window, 3, 1) # (8,3, 1) = (feature, sliding_window, 1)\n",
    "    \n",
    "    inputs=Input(shape=input_shape)\n",
    "    \n",
    "    x=ResBlock(x = inputs,filters=32,kernel_size=kernel_size,dilation_rate=1)\n",
    "    x = Dropout(0.2) (x)\n",
    "    x=ResBlock(x,filters=32,kernel_size=kernel_size,dilation_rate=2)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x=ResBlock(x,filters=16,kernel_size=kernel_size,dilation_rate=4)\n",
    "    \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(utils.numActions, activation='softmax')(x)\n",
    "    model=Model(inputs=inputs,outputs=x)\n",
    "         \n",
    "    model.summary()\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "     \n",
    "    return model\n",
    "\n",
    "\n",
    "def action_frames(y_pred_argmax, ratio_display = False):\n",
    "    action_label =  {\"sit\": 0, \"sit-stand\": 1,  \"walk\": 2, \"turn\": 3,  \"stand-sit\": 4}\n",
    "    action_cnt =  {\"total_frames\": 0,\"sit\": 0, \"sit-stand\": 0,  \"walk\": 0, \"turn\": 0,\"stand-sit\": 0}\n",
    "\n",
    "    for i in range(len(y_pred_argmax)):\n",
    "        action_cnt[\"total_frames\"] +=1\n",
    "        if y_pred_argmax[i] == action_label[\"sit\"]:\n",
    "            action_cnt[\"sit\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"sit-stand\"]:\n",
    "            action_cnt[\"sit-stand\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"walk\"]:\n",
    "            action_cnt[\"walk\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"turn\"]:\n",
    "            action_cnt[\"turn\"] +=1\n",
    "        \n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"stand-sit\"]:\n",
    "            action_cnt[\"stand-sit\"] +=1\n",
    "        \n",
    "    if ratio_display:     \n",
    "        print(\"[ Action Ratio ]\")\n",
    "        print(\"\\t [ 0 - Sit] ratio of sit: \",  action_cnt[\"sit\"]/action_cnt[\"total_frames\"] )\n",
    "        print(\"\\t [ 1 - sit-stand] ratio of sit-stand: \",  action_cnt[\"sit-stand\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 2 - walk] ratio of walk: \",  action_cnt[\"walk\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 3 - turn] ratio of turn: \",  action_cnt[\"turn\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 4 - stand-sit] ratio of stand-sit: \",  action_cnt[\"stand-sit\"]/action_cnt[\"total_frames\"])\n",
    "    return action_cnt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "royal-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dtw import dtw\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "act_dict = {\"sit\":0, \"sit-to-stand\":1, \"walk\": 2, \"turn\": 3, \"stand-to-sit\":4}\n",
    "answer_y = [act_dict[\"sit\"], act_dict[\"sit-to-stand\"], act_dict[\"walk\"], act_dict[\"turn\"], act_dict[\"walk\"], act_dict[\"stand-to-sit\"], act_dict[\"sit\"] ]\n",
    "target_names = [\"sit\", \"sit-to-stand\", \"walk\", \"turn\", \"stand-to-sit\"]\n",
    "\n",
    "\n",
    "def post_process_DTW(time_sec, test_y, pred_y, answer_y, title = \"normal_20\"):\n",
    "    dtw_index ={\"walk\":4, \"stand-to-sit\": 5, \"sit\": 6}\n",
    "    dtw_alignment  = dtw(pred_y,answer_y, keep_internals=True)\n",
    "    \n",
    "    pred_subTask_sec = np.zeros(5)\n",
    "    true_subTask_sec = np.zeros(5)\n",
    "    \n",
    "    pred_subTask_frame = np.zeros(5, np.uint64)\n",
    "    true_subTask_frame = np.zeros(5, np.uint64)\n",
    "\n",
    "    report_time= open(MODEL_SAVE_FOLDER_PATH + \"/Report_frameSplit_Results_\" + str( title)  +\".txt\",'w+t')\n",
    "    # ----------- Time split ... Frame Index\n",
    "    pred_splitFrame =  np.where( np.abs(dtw_alignment.index2[1:]-dtw_alignment.index2[:-1] ) > 0)[0] \n",
    "    true_splitFrame = np.where( np.abs(test_y[1:]-test_y[:-1] ) > 0)[0] + 8 # lookback_window\n",
    "\n",
    "    print(\"\\npred_splitFrame: {0} \\n--> true_splitFrame: {1}\".format(pred_splitFrame,true_splitFrame), file = report_time)\n",
    "    \n",
    "    for i in range(5):\n",
    "        pred_subTask_sec[i] = time_sec[ pred_splitFrame[i+1]] - time_sec[ pred_splitFrame[i]]  # sit ~ sit-to-stand (sit-to-stand)\n",
    "        true_subTask_sec[i] = time_sec[ true_splitFrame[i+1]] - time_sec[ true_splitFrame[i]]     \n",
    "        \n",
    "        pred_subTask_frame[i] = pred_splitFrame[i+1] - pred_splitFrame[i]\n",
    "        true_subTask_frame[i] = true_splitFrame[i+1] - true_splitFrame[i]\n",
    "    print(\"\\npred subTask frame: {0} \\n--> true subTask frame: {1}\".format(pred_subTask_frame , true_subTask_frame), file = report_time)\n",
    "    \n",
    "\n",
    "  \n",
    "        \n",
    "    # ---- Total Time\n",
    "    pred_totalTime = np.sum(pred_subTask_sec)\n",
    "    true_totalTime = np.sum(true_subTask_sec)\n",
    "    print(\"\\nPred SubTask Time: \", pred_subTask_sec, \"\\t\\t Pred Total Time\", pred_totalTime, file = report_time)\n",
    "    print(\"--> True SubTask Time: \", true_subTask_sec, \"\\t\\t True Total Time\", true_totalTime, file = report_time)\n",
    " \n",
    "    #----------- Error---------\n",
    "    print(\"\\n\\n[SubTask Time Error] true - pred (sec): \", np.abs(np.round(true_subTask_sec - pred_subTask_sec,4)), file = report_time)\n",
    "    print(\"[Total Time Error] true - pred (sec): \" , np.abs(np.round(true_totalTime - pred_totalTime,4)) , file = report_time)\n",
    "    report_time.close()\n",
    "    \n",
    "    # ------------ Index 4,5,6 --> 2,4,0 으로 변경 \n",
    "    results =[]\n",
    "    for elements in dtw_alignment.index2:\n",
    "        if elements == dtw_index[\"walk\"]: \n",
    "            elements = act_dict[\"walk\"]\n",
    "        elif elements == dtw_index[\"stand-to-sit\"]:  \n",
    "            elements = act_dict[\"stand-to-sit\"]\n",
    "        elif elements == dtw_index[\"sit\"]:  \n",
    "            elements =  act_dict[\"sit\"]     \n",
    "        results.append(elements)\n",
    "\n",
    "\n",
    "    # 간혹.. dtw 결과가 1개 더 많이 나올 때 있음.. 걍 뒤에 하나 지워버려..\n",
    "    if len(dtw_alignment.index2) > len(pred_y):\n",
    "        new_result = results[:-1]\n",
    "    else:\n",
    "        new_result = results\n",
    "        \n",
    "    return dtw_alignment, new_result, true_subTask_frame, true_totalTime, pred_subTask_frame, pred_totalTime\n",
    "\n",
    "\n",
    "def plot_postResults(answer_y, pred_y, results, title=\"dtw\"):\n",
    "    fig = plt.figure(figsize=(18,12))\n",
    "    \n",
    "    Gnd_fig = fig.add_subplot(2,3,1)\n",
    "    Gnd_fig.set_title(\" Ground Truth \")\n",
    "    Gnd_fig.plot(answer_y, \"g\", label = \"Ground Truth\")\n",
    "    plt.legend()\n",
    "    \n",
    "    pred_fig = fig.add_subplot(2,3,2)\n",
    "    pred_fig.set_title(\" Prediction Result (Before DTW) \")\n",
    "    pred_fig.plot(pred_y, \"b\", label=\" pred_y (before DTW)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    dtw_fig = fig.add_subplot(2,3,3)\n",
    "    dtw_fig.set_title(\" PostProcessing Results (DTW) \")\n",
    "    dtw_fig.plot(results, \"r\",  label=\"dtw_result\")\n",
    "    plt.legend()\n",
    "    \n",
    "    gnd_pred_fig = fig.add_subplot(2,3,4)\n",
    "    gnd_pred_fig.set_title(\" Comparison: Ground Truth vs Prediction\")\n",
    "    gnd_pred_fig.plot(answer_y, \"g\", label=\"Ground Truth\") # linewidth=3\n",
    "    gnd_pred_fig.plot(pred_y, \"b--\", label=\" pred_y (before DTW)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    pred_dtw_fig = fig.add_subplot(2,3,5)\n",
    "    pred_dtw_fig.set_title(\" Comparison: Prediction vs DTW alignment\")\n",
    "    pred_dtw_fig.plot(pred_y, \"b--\", label=\" pred_y (before DTW)\") # linewidth=3\n",
    "    pred_dtw_fig.plot(results, \"r\", label=\" dtw_result\")\n",
    "    plt.legend()\n",
    "    \n",
    "    gnd_dtw_fig = fig.add_subplot(2,3,6)\n",
    "    gnd_dtw_fig.set_title(\" Comparison: Ground Truth vs DTW alignment \")\n",
    "    gnd_dtw_fig.plot(answer_y, \"g--\", label=\"Ground Truth\")\n",
    "    gnd_dtw_fig.plot(results, \"r\", label=\"dtw_result\")  # linewidth=3\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/DTW_Comparison_\" + title  + \".png\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_testdata(pelvis_csv, lookback_window = 8):\n",
    "    test_x = np.zeros((0, utils.lookback_window,  utils.numPelvis,1 ))\n",
    "    test_y = np.zeros((0,  utils.numActions)) \n",
    "   \n",
    "    timstamp_ms, lpfPelvis_x, lpfPelvis_y, lpfPelvis_z, oneHot_actionList  =  utils.getLPF_PelvisData(pelvis_csv)  # raw                 \n",
    "    pelvisData = np.array([lpfPelvis_x, lpfPelvis_y, lpfPelvis_z]).T  # (335,3) \n",
    "    actionData = np.array(oneHot_actionList)    # # (335, 5)\n",
    "                            \n",
    "    blockTime, blockPelvis, blockLable =  utils.sliding_window(timstamp_ms, pelvisData, actionData)\n",
    "    test_x = np.append(test_x, blockPelvis, axis = 0 )\n",
    "    test_y = np.append(test_y, blockLable, axis = 0 )    \n",
    "\n",
    "    return blockTime, test_x, test_y\n",
    "\n",
    "\n",
    "def barplot_splitResults(pred_subTask_frame, true_subTask_frame, pred_totalTime, true_totalTime, title= \"normal_20\"):\n",
    "    subTask_label = ['sit-to-stand', 'walk', 'turn', 'walk-back', 'sit-back']\n",
    "    actIndex = np.arange(len( [\"Pred\", \"Truth\"]))\n",
    "    color = ['b','g','r','c','y']\n",
    "    \n",
    "        \n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.title('TUG sub-Task segmentation Results\\n', loc='center')\n",
    "    plt.xlabel('frames')\n",
    "    plt.yticks(actIndex,  [\"Model Pred\", \"Ground Truth\"])\n",
    "    plt.text(10, 0.5, \"True totalTime: \" + str(np.round(true_totalTime,4)) + \"  ,   Pred totalTime: \" + str(np.round(pred_totalTime,4)), fontsize=13)\n",
    "    plt.text(10,-0.8, \"TotalTime Error (sec): \" + str(np.round(np.abs(true_totalTime-pred_totalTime),4)), fontsize=13, fontweight='bold')\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(color)):\n",
    "        rects = plt.barh(actIndex,  [pred_subTask_frame[i] , true_subTask_frame[i]] , color = color[i], left = [ np.sum(np.sum(pred_subTask_frame[:i])),  np.sum(np.sum(true_subTask_frame[:i]))] )\n",
    " \n",
    "        for j, rect in enumerate(rects):\n",
    "            if j is 0:  # --- pred\n",
    "                h1 = rect.get_height()\n",
    "                w1 = rect.get_width()\n",
    "                plt.text(rect.get_x() + rect.get_width() / 2., h1 / 1., \"%d\" % true_subTask_frame[i], color=\"k\", fontsize=12, fontweight=\"bold\")         \n",
    "\n",
    "            else:  # -- true \n",
    "                h2 = rect.get_height()\n",
    "                w2 = rect.get_width()\n",
    "                plt.text(rect.get_x() + w2/ 2., (h2-h1) / 1.,\"%d\" % pred_subTask_frame[i], color=\"k\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        \n",
    "\n",
    "    plt.legend(subTask_label,  bbox_to_anchor=([1, 1, 0, 0]))                         \n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/BarGragh_FrameSplit_\" + title  + \".png\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "friendly-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0:0.303, 1:1.0, 2: 0.144, 3:0.487, 4: 0.7}\n",
    "# class_weight = {0:1. , 1:3.3, 2: 0.4752, 3:1.6071, 4: 2.31} #  x 0.303 기준으로 다 배수  \n",
    "\n",
    "batch_size = 512\n",
    "epochs = 100000\n",
    "learning_rate = 0.00001 #0.0001\n",
    "patience = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bulgarian-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8, 3, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 3, 32)     320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 3, 32)     9248        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 3, 32)     320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8, 3, 32)     0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 8, 3, 32)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 8, 3, 32)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 3, 32)     9248        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 3, 32)     9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 8, 3, 32)     0           conv2d_4[0][0]                   \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 3, 32)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8, 3, 32)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 3, 16)     4624        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 3, 16)     2320        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 3, 16)     4624        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 3, 16)     0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 3, 16)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            1925        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 41,877\n",
      "Trainable params: 41,877\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: /DockerProjects/walkCAM/tug/temporalCNN_wLabel/KResults/LSTM2HMM_saveResults_illness_KYH/P20_norm20_Kfold0_20210505-Time_17-54/Model/Origin_TCN_model/assets\n",
      "Epoch 1/100000\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv2d/Conv2D (defined at <ipython-input-7-fc026b12a9a0>:32) ]] [Op:__inference_train_function_3301]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc026b12a9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_FOLDER_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/Origin_TCN_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# hist = model.fit(train_x,train_y, batch_size= batch_size, epochs=epochs,verbose=2, validation_data= (valid_x, valid_y),   callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_early_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv2d/Conv2D (defined at <ipython-input-7-fc026b12a9a0>:32) ]] [Op:__inference_train_function_3301]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lookback_window = 8\n",
    "\n",
    "for selected_foldNum in range(Kfold_num):\n",
    "    train_subjects = trainFold[selected_foldNum]\n",
    "    test_subjects = testFold[selected_foldNum]  \n",
    "\n",
    "    total_Trials, train_x, train_y, valid_x, valid_y, test_x, test_y =  utils.readLPF_createDataset(specific_group, train_subjects, test_subjects, Kfold )\n",
    "    print(\"train_x: \", train_x.shape,\" , train_y: \", train_y.shape)\n",
    "    print(\"valid_x: \",valid_x.shape, \" , valid_y: \",valid_y.shape)\n",
    "    print(\"test_x: \",test_x.shape, \"  , test_y: \",test_y.shape)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    \n",
    "    ModelTime =  time.strftime('%Y%m%d-Time_%H-%M', time.localtime(time.time())) \n",
    "    MODEL_SAVE_FOLDER_PATH =\"/DockerProjects/walkCAM/tug/temporalCNN_wLabel/KResults\" + expertFolder +\"/P\" +str(patience) + \"_\"+str(selected_type)+\"_Kfold\"+str(selected_foldNum)+\"_\"+ ModelTime +  \"/Model\" \n",
    "    print(MODEL_SAVE_FOLDER_PATH)\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.makedirs(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + \"-\"+ selected_type +\"-lr:\"+ str(learning_rate) +\"-p:\" +str(patience)+ \"_{epoch:04d} -- {loss:.4f}.hdf5\"\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"loss\", verbose=1, save_best_only=True)\n",
    "    cb_early_stopping = EarlyStopping(monitor=\"loss\", patience=patience)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "    model = TCN(optimizer)\n",
    "    model.save(MODEL_SAVE_FOLDER_PATH+\"/Origin_TCN_model\")\n",
    "    # hist = model.fit(train_x,train_y, batch_size= batch_size, epochs=epochs,verbose=2, validation_data= (valid_x, valid_y),   callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "    hist = model.fit(train_x,train_y, batch_size= batch_size, epochs=epochs,verbose=1, callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "    eval_result = model.evaluate(test_x, test_y,batch_size= batch_size,verbose=1)\n",
    "    \n",
    "    reportd_subjects = open(MODEL_SAVE_FOLDER_PATH + \"/\" + str(selected_type)+ \"_subjectLists_Kfold_\" + str(selected_foldNum) + \".txt\",'w+t')\n",
    "    \n",
    "    for i in range(Kfold_num): \n",
    "        print(\"\\t [\", i, \"-fold] Train :\", trainFold[i], \"\\n\\t\\t\\t, Test Fold: \", testFold[i], file = reportd_subjects)\n",
    "    \n",
    "    print(\"selected_foldNum: \", selected_foldNum, file = reportd_subjects)\n",
    "    print(\"[Train subjects]\\n\\t\", train_subjects , file =reportd_subjects )\n",
    "    print(\"[Test subjects]\\n\\t\", test_subjects , file =reportd_subjects )\n",
    "    print('test_loss:',eval_result[0],'- test_acc:',eval_result[1], file =reportd_subjects)\n",
    "    reportd_subjects.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('test_loss:',eval_result[0],'- test_acc:',eval_result[1])\n",
    "    clear_output()\n",
    "    \n",
    "    \n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "    acc_ax.set_ylabel('accuracy')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/lr_\"+str(learning_rate)+\"_patience_\"+ str(patience)+\"_Loss_\"+str(np.round(eval_result[1],4))+\".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "\n",
    "\n",
    "    y_pred = model.predict(test_x)\n",
    "    y_pred_onehot = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "    y_test_argmax = np.argmax(test_y ,axis=1) \n",
    "    y_pred_argmax = np.argmax(y_pred ,axis=1)\n",
    "\n",
    "    y_pred_frames = action_frames(y_pred_argmax, ratio_display= True)\n",
    "    y_test_frames = action_frames(y_test_argmax,  ratio_display= True)   \n",
    "    print(\"--->  # of Each Action Frames  \\n\\t y_pred: {0}, \\n\\t y_test: {1}\".format(y_pred_frames, y_test_frames))\n",
    "    \n",
    "    clear_output()\n",
    "\n",
    "    \n",
    "    kappa = cohen_kappa_score(y_test_argmax, y_pred_argmax)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "    print(classification_report(y_test_argmax, y_pred_argmax))\n",
    "    conf_matrix = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "    print(\"\\n---> Confusion Matrix \\n\" ,conf_matrix) # sit, sit-stand, walking, turning, stand-sit\n",
    "\n",
    "    plt.figure(figsize = (15,7) )\n",
    "    sns.heatmap(conf_matrix, annot=True)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/pred_Confusion_Matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    test_normal_20 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm20/jek/02/Originact5_lpf_labeled_jek_02.csv\"\n",
    "    test_normal_20_time, test_normal_20_x, test_normal_20_y_onehot = create_testdata(test_normal_20) \n",
    "    test_normal_20_y = np.argmax(test_normal_20_y_onehot, axis=1).reshape(-1) # GND one-hot decode\n",
    "\n",
    "    pred_normal_20_y_onehot = model.predict(test_normal_20_x) \n",
    "    pred_normal_20_y = np.argmax(pred_normal_20_y_onehot, axis=1) #  # pred one-hot decode\n",
    "\n",
    "    title = \"normal_20\"\n",
    "    normal_20_alignment, normal_20_results, true_subTask_frame_20, true_totalTime_20, pred_subTask_frame_20, pred_totalTime_20 = post_process_DTW(test_normal_20_time, test_normal_20_y, pred_normal_20_y, answer_y, title = title)\n",
    "\n",
    "    # --------------- DTW Result Visualization ------------------------ #\n",
    "    normal_20_alignment.plot(type=\"threeway\")\n",
    "    plot_postResults(test_normal_20_y, pred_normal_20_y, normal_20_results, title = title)\n",
    "    norm_20_eval = model.evaluate(test_normal_20_x, test_normal_20_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "    report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_20.txt\",'w+t')\n",
    "    print(\"[Before DTW] Before_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, pred_normal_20_y, target_names=target_names), file = report_dtw)\n",
    "    print(\"------------------------------\\n\", file = report_dtw)\n",
    "    print(\"[After DTW] After_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, normal_20_results, target_names=target_names), file = report_dtw)\n",
    "    report_dtw.close()\n",
    "\n",
    "    # # ----------- Visualize in cell\n",
    "    print(\"[Before DTW] Before_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, pred_normal_20_y, target_names=target_names))\n",
    "    print(\"------------------------------\\n\")\n",
    "    print(\"[After DTW] After_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, normal_20_results, target_names=target_names))\n",
    "\n",
    "\n",
    "    print(\" [True]\", true_subTask_frame_20, true_totalTime_20,\"\\n [Pred]\", pred_subTask_frame_20, pred_totalTime_20) \n",
    "    barplot_splitResults(pred_subTask_frame_20, true_subTask_frame_20,pred_totalTime_20, true_totalTime_20, title=title)\n",
    "    \n",
    "    clear_output()\n",
    "        \n",
    "    test_normal_30 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm20/jdh/03/Originact5_lpf_labeled_jdh_03.csv\"\n",
    "    test_normal_30_time, test_normal_30_x, test_normal_30_y_onehot = create_testdata(test_normal_30) \n",
    "    test_normal_30_y = np.argmax(test_normal_30_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "    pred_normal_30_onehot = model.predict(test_normal_30_x)\n",
    "    pred_normal_30_y = np.argmax(pred_normal_30_onehot, axis=1)\n",
    "\n",
    "    title = \"normal_30\"\n",
    "    normal_30_alignment, normal_30_results,  true_subTask_frame_30, true_totalTime_30, pred_subTask_frame_30, pred_totalTime_30 = post_process_DTW(test_normal_30_time, test_normal_30_y, pred_normal_30_y, answer_y, title=title)\n",
    "\n",
    "    # --------------- DTW Result Visualization ------------------------ #\n",
    "    normal_30_alignment.plot(type=\"threeway\")\n",
    "    plot_postResults(test_normal_30_y, pred_normal_30_y, normal_30_results, title=title)\n",
    "    norm_30_eval = model.evaluate(test_normal_30_x, test_normal_30_y_onehot,batch_size= batch_size,verbose=2)\n",
    "\n",
    "    report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_30.txt\",'w+t')\n",
    "    print(\"[Before DTW] Before_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, pred_normal_30_y, target_names=target_names), file = report_dtw)\n",
    "    print(\"------------------------------\\n\", file = report_dtw)\n",
    "    print(\"[After DTW] After_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, normal_30_results, target_names=target_names), file = report_dtw)\n",
    "    report_dtw.close()\n",
    "\n",
    "    # # ----------- Visualize in cell\n",
    "    print(\"[Before DTW] Before_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, pred_normal_30_y, target_names=target_names))\n",
    "    print(\"------------------------------\\n\")\n",
    "    print(\"[After DTW] After_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, normal_30_results, target_names=target_names))\n",
    "\n",
    "    print(\" [True]\", true_subTask_frame_30, true_totalTime_30,\"\\n [Pred]\", pred_subTask_frame_30, pred_totalTime_30) \n",
    "    barplot_splitResults(pred_subTask_frame_30, true_subTask_frame_30, pred_totalTime_30,true_totalTime_30, title=title)\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "#     test_normal_60 = \"/DockerProjects/Dataset/TUG/trainSet/HMM_saveResults_최윤정/0_sideView/2021_02_09 TUG/UMS_TUG/05/Originact5_lpf_labeled_UMS_TUG_05.csv\"\n",
    "    test_normal_60 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm60/LBL_TUG/05/Originact5_lpf_labeled_LBL_TUG_05.csv\"\n",
    "    test_normal_60_time, test_normal_60_x, test_normal_60_y_onehot = create_testdata(test_normal_60) \n",
    "    test_normal_60_y = np.argmax(test_normal_60_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "\n",
    "    pred_normal_60_onehot = model.predict(test_normal_60_x)\n",
    "    pred_normal_60_y = np.argmax(pred_normal_60_onehot, axis=1).reshape(-1) \n",
    "\n",
    "    title = \"normal_60\"\n",
    "    normal_60_alignment, normal_60_results, true_subTask_frame_60, true_totalTime_60, pred_subTask_frame_60, pred_totalTime_60 = post_process_DTW(test_normal_60_time, test_normal_60_y, pred_normal_60_y, answer_y, title=title)\n",
    "\n",
    "\n",
    "    # --------------- DTW Result Visualization ------------------------ #\n",
    "    normal_60_alignment.plot(type=\"threeway\")\n",
    "    plot_postResults(test_normal_60_y, pred_normal_60_y, normal_60_results, title=title)\n",
    "    norm_60_eval = model.evaluate(test_normal_60_x, test_normal_60_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "    report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_60.txt\",'w+t')\n",
    "    print(\"[Before DTW] Before DTW Norm 60 \\n\", classification_report(test_normal_60_y, pred_normal_60_y, target_names=target_names), file = report_dtw)\n",
    "    print(\"------------------------------\\n\",file = report_dtw)\n",
    "    print(\"[After DTW] After_DTW_Norm_60 \\n\", classification_report(test_normal_60_y, normal_60_results, target_names=target_names), file =   report_dtw)\n",
    "    report_dtw.close()\n",
    "\n",
    "    # ----------- Visualize in cell\n",
    "    print(\"[Before DTW] Before DTW Norm 60 \\n\", classification_report(test_normal_60_y, pred_normal_60_y, target_names=target_names))\n",
    "    print(\"------------------------------\\n\")\n",
    "    print(\"[After DTW] After_DTW_Norm_60 \\n\", classification_report(test_normal_60_y, normal_60_results, target_names=target_names))\n",
    "\n",
    "    print(\" [True]\", true_subTask_frame_60, true_totalTime_60,\"\\n [Pred]\", pred_subTask_frame_60, pred_totalTime_60) \n",
    "    barplot_splitResults(pred_subTask_frame_60, true_subTask_frame_60, pred_totalTime_60, true_totalTime_60, title=title)\n",
    "    \n",
    "    clear_output()\n",
    "\n",
    "#     test_stroke_1 =  \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/stroke/JDW_tug/01/Originact5_lpf_labeled_JDW_tug_01.csv\"\n",
    "    test_stroke_1 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/stroke/SHW_TUG/04/Originact5_lpf_labeled_SHW_TUG_04.csv\"\n",
    "    test_normal_stroke_time, test_stroke_1_x, test_stroke_1_y_onehot = create_testdata(test_stroke_1)  \n",
    "    test_stroke_1_y = np.argmax(test_stroke_1_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "    pred_stroke_1_one_hot = model.predict(test_stroke_1_x)\n",
    "    pred_stroke_1_y = np.argmax(pred_stroke_1_one_hot, axis=1)\n",
    "\n",
    "    title=\"stroke_1\"\n",
    "    stroke_1_alignment, stroke_1_results, true_subTask_frame_stroke, true_totalTime_stroke, pred_subTask_frame_stroke, pred_totalTime_stroke = post_process_DTW(test_normal_stroke_time, test_stroke_1_y, pred_stroke_1_y, answer_y,  title=title)\n",
    "\n",
    "\n",
    "    # --------------- DTW Result Visualization ------------------------ #\n",
    "    stroke_1_alignment.plot(type=\"threeway\")\n",
    "    plot_postResults(test_stroke_1_y, pred_stroke_1_y, stroke_1_results, title=title)\n",
    "    stroke_1_eval = model.evaluate(test_stroke_1_x, test_stroke_1_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "    report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_stroke_1.txt\",'w+t')\n",
    "    print(\"[Before DTW] Before DTW stroke_1 \\n\", classification_report(test_stroke_1_y, pred_stroke_1_y, target_names=target_names), file = report_dtw)\n",
    "    print(\"------------------------------\\n\", file = report_dtw)\n",
    "    print(\"[After DTW] After DTW stroke_1 \\n\", classification_report(test_stroke_1_y, stroke_1_results, target_names=target_names), file = report_dtw)\n",
    "    report_dtw.close()\n",
    "\n",
    "    # # ----------- Visualize in cell\n",
    "    print(\"[Before DTW] Before DTW stroke_1 \\n\", classification_report(test_stroke_1_y, pred_stroke_1_y, target_names=target_names))\n",
    "    print(\"------------------------------\\n\")\n",
    "    print(\"[After DTW] After DTW stroke_1 \\n\", classification_report(test_stroke_1_y, stroke_1_results, target_names=target_names))\n",
    "\n",
    "    print(\" [True]\", true_subTask_frame_stroke, true_totalTime_stroke,\"\\n [Pred]\", pred_subTask_frame_stroke, pred_totalTime_stroke) \n",
    "    barplot_splitResults(pred_subTask_frame_stroke, true_subTask_frame_stroke, pred_totalTime_stroke, true_totalTime_stroke, title=title)\n",
    "\n",
    "    clear_output()\n",
    "    \n",
    "    test_stroke_tool = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/stroke/KNG_TUG/03/Originact5_lpf_labeled_KNG_TUG_03.csv\"\n",
    "    test_stroke_tool_time, test_stroke_tool_x, test_stroke_tool_y_onehot = create_testdata(test_stroke_tool) \n",
    "    test_stroke_tool_y = np.argmax(test_stroke_tool_y_onehot, axis=1).reshape(-1) #one-hot decode\n",
    "\n",
    "    pred_stroke_tool_onehot = model.predict(test_stroke_tool_x)\n",
    "    pred_stroke_tool_y = np.argmax(pred_stroke_tool_onehot, axis=1)\n",
    "\n",
    "    title = \"stroke_tool\"\n",
    "    stroke_tool_alignment, stroke_tool_results, true_subTask_frame_stroke_tool, true_totalTime_stroke_tool, pred_subTask_frame_stroke_tool, pred_totalTime_stroke_tool = post_process_DTW(test_stroke_tool_time, test_stroke_tool_y, pred_stroke_tool_y, answer_y, title=title)\n",
    "\n",
    "\n",
    "    # --------------- DTW Result Visualization ------------------------ #\n",
    "    stroke_tool_alignment.plot(type=\"threeway\")\n",
    "    plot_postResults(test_stroke_tool_y, pred_stroke_tool_y, stroke_tool_results, title=title)\n",
    "    stroke_tool_eval = model.evaluate(test_stroke_tool_x, test_stroke_tool_y_onehot,batch_size= batch_size,verbose=2)\n",
    "\n",
    "    report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_stroke_tool.txt\",'w+t')\n",
    "    print(\"[Before DTW] Before DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, pred_stroke_tool_y, target_names=target_names), file = report_dtw)\n",
    "    print(\"------------------------------\\n \", file = report_dtw)\n",
    "    print(\"[After DTW] After DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, stroke_tool_results, target_names=target_names), file = report_dtw)\n",
    "    report_dtw.close()\n",
    "\n",
    "    # # ----------- Visualize in cell\n",
    "    print(\"[Before DTW] Before DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, pred_stroke_tool_y, target_names=target_names))\n",
    "    print(\"------------------------------\\n \")\n",
    "    print(\"[After DTW] After DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, stroke_tool_results, target_names=target_names))\n",
    "\n",
    "    print(\" [True]\", true_subTask_frame_stroke_tool, true_totalTime_stroke_tool,\"\\n [Pred]\", pred_subTask_frame_stroke_tool, pred_totalTime_stroke_tool) \n",
    "    barplot_splitResults(pred_subTask_frame_stroke_tool, true_subTask_frame_stroke_tool, pred_totalTime_stroke_tool, true_totalTime_stroke_tool, title=title)\n",
    "    \n",
    "    \n",
    "    clear_output()\n",
    "    src = \"/DockerProjects/walkCAM/tug/temporalCNN_wLabel/Results\" + expertFolder +\"/\" + str(selected_type) +\"_P\" +str(patience)+\"_\" + ModelTime \n",
    "    dst = \"/DockerProjects/walkCAM/tug/temporalCNN_wLabel/Results\" + expertFolder +\"/\" + str(selected_type) +\"_P\" +str(patience)+\"_\" + ModelTime + \"_loss-\" + str(np.round(eval_result[1],4))\n",
    "    print(dst)\n",
    "    os.rename(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-tribune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-detective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-slovak",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-administrator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-brazilian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-coordinator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-chancellor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "significant-apollo",
   "metadata": {},
   "source": [
    "lookback_window = 8\n",
    "\n",
    "selected_foldNum = 0\n",
    "train_subjects = trainFold[selected_foldNum]\n",
    "test_subjects = testFold[selected_foldNum]  \n",
    "\n",
    "\n",
    "total_Trials, train_x, train_y, valid_x, valid_y, test_x, test_y =  utils.readLPF_createDataset(specific_group, train_subjects, test_subjects, Kfold )\n",
    "print(\"train_x: \", train_x.shape,\" , train_y: \", train_y.shape)\n",
    "print(\"valid_x: \",valid_x.shape, \" , valid_y: \",valid_y.shape)\n",
    "print(\"test_x: \",test_x.shape, \"  , test_y: \",test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-albert",
   "metadata": {},
   "source": [
    "### Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-reporter",
   "metadata": {},
   "source": [
    "from keras.layers import Conv1D, Input, Activation, Flatten, Dense, Conv2D\n",
    "from keras.layers import BatchNormalization, add , Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-timber",
   "metadata": {},
   "source": [
    "ModelTime =  time.strftime('%Y%m%d-Time_%H-%M', time.localtime(time.time())) +  \"/Model\" \n",
    "MODEL_SAVE_FOLDER_PATH =\"/DockerProjects/walkCAM/tug/0_temporalCNN_wLabel/Results\" + expertFolder +\"/\" + ModelTime\n",
    "\n",
    "print(MODEL_SAVE_FOLDER_PATH)\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.makedirs(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + \"LookBack- \"+ str(lookback_window) +\" Origin\" + \"{epoch:04d} ---- {val_loss: .4f}.hdf5\"\n",
    "patience = 50\n",
    "\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor=\"val_loss\", patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-victorian",
   "metadata": {},
   "source": [
    "class_weight = {0:0.303, 1:1.0, 2: 0.144, 3:0.487, 4: 0.7}\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 100000\n",
    "learning_rate = 0.00001\n",
    "\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-wrestling",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ModelTime =  time.strftime('%Y%m%d-Time_%H-%M', time.localtime(time.time())) +  \"/Model\" \n",
    "MODEL_SAVE_FOLDER_PATH =\"/DockerProjects/walkCAM/tug/temporalCNN_wLabel/KResults\" + expertFolder +\"/P\" +str(patience) + \"_\"+str(selected_type)+\"_Kfold\"+str(selected_foldNum)+\"_\"+ ModelTime\n",
    "\n",
    "print(MODEL_SAVE_FOLDER_PATH)\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.makedirs(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + \"-\"+ selected_type +\"-lr:\"+ str(learning_rate) +\"-p:\" +str(patience)+ \"_{epoch:04d} -- {val_loss:.4f}.hdf5\"\n",
    "\n",
    "\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"loss\", verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor=\"loss\", patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-skill",
   "metadata": {},
   "source": [
    " #Residual block :: https://roadcom.tistory.com/95\n",
    "def ResBlock(x,filters,kernel_size,dilation_rate):\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate,activation='relu')(x) #first convolution\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate)(r) #Second convolution\n",
    "    if x.shape[-1]==filters:\n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 일치할 경우 단순 add 연산만 진행하는 블록 = identity block\n",
    "        shortcut = x  # identity block \n",
    "    else: \n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 다를 경우 shortcut path 를 적절히 변환\n",
    "        # 즉, projection 을 통해 channel 을 맞춰주는 작업이(projection shortcut) 추가되기에 이를 convolution block 이라함\n",
    "        shortcut=Conv2D(filters,kernel_size=kernel_size,padding='same')(x) \n",
    "    o=add([r,shortcut])\n",
    "    o=Activation('relu')(o) \n",
    "    return o\n",
    " \n",
    " #Sequence Model\n",
    "def TCN(optimizer='adam'):\n",
    "    kernel_size = (3,3)\n",
    "    input_shape =  (lookback_window, 3, 1) # (8,3, 1) = (feature, sliding_window, 1)\n",
    "    \n",
    "    inputs=Input(shape=input_shape)\n",
    "    \n",
    "    x=ResBlock(x = inputs,filters=32,kernel_size=kernel_size,dilation_rate=1)\n",
    "    x = Dropout(0.2) (x)\n",
    "    x=ResBlock(x,filters=32,kernel_size=kernel_size,dilation_rate=2)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x=ResBlock(x,filters=16,kernel_size=kernel_size,dilation_rate=4)\n",
    "    \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(numActions, activation='softmax')(x)\n",
    "    model=Model(inputs=inputs,outputs=x)\n",
    "         \n",
    "    model.summary()\n",
    "        \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-nomination",
   "metadata": {},
   "source": [
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "model = TCN(optimizer)\n",
    "model.save(MODEL_SAVE_FOLDER_PATH+\"/Origin_TCN_model\")\n",
    "# hist = model.fit(train_x,train_y, batch_size= batch_size, epochs=epochs,verbose=2, validation_data= (valid_x, valid_y),   callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "hist = model.fit(train_x,train_y, batch_size= batch_size, epochs=epochs,verbose=1, callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "eval_result = model.evaluate(test_x, test_y,batch_size= batch_size,verbose=1)\n",
    "\n",
    "print('test_loss:',eval_result[0],'- test_acc:',eval_result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-scope",
   "metadata": {},
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "# acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/lr_\"+str(learning_rate)+\"_patience_\"+ str(patience)+\"_Loss_\"+str(np.round(eval_result[1],4))+\".png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-boost",
   "metadata": {},
   "source": [
    "def action_frames(y_pred_argmax, ratio_display = False):\n",
    "    action_label =  {\"sit\": 0, \"sit-stand\": 1,  \"walk\": 2, \"turn\": 3,  \"stand-sit\": 4}\n",
    "    action_cnt =  {\"total_frames\": 0,\"sit\": 0, \"sit-stand\": 0,  \"walk\": 0, \"turn\": 0,\"stand-sit\": 0}\n",
    "\n",
    "    for i in range(len(y_pred_argmax)):\n",
    "        action_cnt[\"total_frames\"] +=1\n",
    "        if y_pred_argmax[i] == action_label[\"sit\"]:\n",
    "            action_cnt[\"sit\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"sit-stand\"]:\n",
    "            action_cnt[\"sit-stand\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"walk\"]:\n",
    "            action_cnt[\"walk\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"turn\"]:\n",
    "            action_cnt[\"turn\"] +=1\n",
    "        \n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"stand-sit\"]:\n",
    "            action_cnt[\"stand-sit\"] +=1\n",
    "        \n",
    "    if ratio_display:     \n",
    "        print(\"[ Action Ratio ]\")\n",
    "        print(\"\\t [ 0 - Sit] ratio of sit: \",  action_cnt[\"sit\"]/action_cnt[\"total_frames\"] )\n",
    "        print(\"\\t [ 1 - sit-stand] ratio of sit-stand: \",  action_cnt[\"sit-stand\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 2 - walk] ratio of walk: \",  action_cnt[\"walk\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 3 - turn] ratio of turn: \",  action_cnt[\"turn\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 4 - stand-sit] ratio of stand-sit: \",  action_cnt[\"stand-sit\"]/action_cnt[\"total_frames\"])\n",
    "    return action_cnt\n",
    "    \n",
    "    \n",
    "    \n",
    "y_pred = model.predict(test_x)\n",
    "y_pred_onehot = np.argmax(y_pred, axis=1)\n",
    "print(y_pred.shape)\n",
    "print(y_pred[200] ,\"===> onehot: \", y_pred_onehot[200])\n",
    "\n",
    "y_test_argmax = np.argmax(test_y ,axis=1) \n",
    "y_pred_argmax = np.argmax(y_pred ,axis=1)\n",
    "print(y_test_argmax.shape, y_pred_argmax.shape)\n",
    "\n",
    "\n",
    "y_pred_frames = action_frames(y_pred_argmax, ratio_display= True)\n",
    "y_test_frames = action_frames(y_test_argmax,  ratio_display= True)   \n",
    "\n",
    "print(\"--->  # of Each Action Frames  \\n\\t y_pred: {0}, \\n\\t y_test: {1}\".format(y_pred_frames, y_test_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-insider",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "kappa = cohen_kappa_score(y_test_argmax, y_pred_argmax)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "print(classification_report(y_test_argmax, y_pred_argmax))\n",
    "conf_matrix = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "print(\"\\n---> Confusion Matrix \\n\" ,conf_matrix) # sit, sit-stand, walking, turning, stand-sit\n",
    "\n",
    "plt.figure(figsize = (15,7) )\n",
    "sns.heatmap(conf_matrix, annot=True)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/pred_Confusion_Matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-premises",
   "metadata": {},
   "source": [
    "## POST processing (DTW) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-christmas",
   "metadata": {},
   "source": [
    "from dtw import dtw\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "act_dict = {\"sit\":0, \"sit-to-stand\":1, \"walk\": 2, \"turn\": 3, \"stand-to-sit\":4}\n",
    "answer_y = [act_dict[\"sit\"], act_dict[\"sit-to-stand\"], act_dict[\"walk\"], act_dict[\"turn\"], act_dict[\"walk\"], act_dict[\"stand-to-sit\"], act_dict[\"sit\"] ]\n",
    "target_names = [\"sit\", \"sit-to-stand\", \"walk\", \"turn\", \"stand-to-sit\"]\n",
    "\n",
    "\n",
    "def post_process_DTW(time_sec, test_y, pred_y, answer_y, title = \"normal_20\"):\n",
    "    dtw_index ={\"walk\":4, \"stand-to-sit\": 5, \"sit\": 6}\n",
    "    dtw_alignment  = dtw(pred_y,answer_y, keep_internals=True)\n",
    "    \n",
    "    pred_subTask_sec = np.zeros(5)\n",
    "    true_subTask_sec = np.zeros(5)\n",
    "    \n",
    "    pred_subTask_frame = np.zeros(5, np.uint64)\n",
    "    true_subTask_frame = np.zeros(5, np.uint64)\n",
    "\n",
    "    report_time= open(MODEL_SAVE_FOLDER_PATH + \"/Report_frameSplit_Results_\" + str( title)  +\".txt\",'w+t')\n",
    "    # ----------- Time split ... Frame Index\n",
    "    pred_splitFrame =  np.where( np.abs(dtw_alignment.index2[1:]-dtw_alignment.index2[:-1] ) > 0)[0] \n",
    "    true_splitFrame = np.where( np.abs(test_y[1:]-test_y[:-1] ) > 0)[0] + 8 # lookback_window\n",
    "\n",
    "    print(\"\\npred_splitFrame: {0} \\n--> true_splitFrame: {1}\".format(pred_splitFrame,true_splitFrame), file = report_time)\n",
    "    \n",
    "    for i in range(5):\n",
    "        pred_subTask_sec[i] = time_sec[ pred_splitFrame[i+1]] - time_sec[ pred_splitFrame[i]]  # sit ~ sit-to-stand (sit-to-stand)\n",
    "        true_subTask_sec[i] = time_sec[ true_splitFrame[i+1]] - time_sec[ true_splitFrame[i]]     \n",
    "        \n",
    "        pred_subTask_frame[i] = pred_splitFrame[i+1] - pred_splitFrame[i]\n",
    "        true_subTask_frame[i] = true_splitFrame[i+1] - true_splitFrame[i]\n",
    "    print(\"\\npred subTask frame: {0} \\n--> true subTask frame: {1}\".format(pred_subTask_frame , true_subTask_frame), file = report_time)\n",
    "    \n",
    "\n",
    "  \n",
    "        \n",
    "    # ---- Total Time\n",
    "    pred_totalTime = np.sum(pred_subTask_sec)\n",
    "    true_totalTime = np.sum(true_subTask_sec)\n",
    "    print(\"\\nPred SubTask Time: \", pred_subTask_sec, \"\\t\\t Pred Total Time\", pred_totalTime, file = report_time)\n",
    "    print(\"--> True SubTask Time: \", true_subTask_sec, \"\\t\\t True Total Time\", true_totalTime, file = report_time)\n",
    " \n",
    "    #----------- Error---------\n",
    "    print(\"\\n\\n[SubTask Time Error] true - pred (sec): \", np.abs(np.round(true_subTask_sec - pred_subTask_sec,4)), file = report_time)\n",
    "    print(\"[Total Time Error] true - pred (sec): \" , np.abs(np.round(true_totalTime - pred_totalTime,4)) , file = report_time)\n",
    "    report_time.close()\n",
    "    \n",
    "    # ------------ Index 4,5,6 --> 2,4,0 으로 변경 \n",
    "    results =[]\n",
    "    for elements in dtw_alignment.index2:\n",
    "        if elements == dtw_index[\"walk\"]: \n",
    "            elements = act_dict[\"walk\"]\n",
    "        elif elements == dtw_index[\"stand-to-sit\"]:  \n",
    "            elements = act_dict[\"stand-to-sit\"]\n",
    "        elif elements == dtw_index[\"sit\"]:  \n",
    "            elements =  act_dict[\"sit\"]     \n",
    "        results.append(elements)\n",
    "\n",
    "\n",
    "    # 간혹.. dtw 결과가 1개 더 많이 나올 때 있음.. 걍 뒤에 하나 지워버려..\n",
    "    if len(dtw_alignment.index2) > len(pred_y):\n",
    "        new_result = results[:-1]\n",
    "    else:\n",
    "        new_result = results\n",
    "        \n",
    "    return dtw_alignment, new_result, true_subTask_frame, true_totalTime, pred_subTask_frame, pred_totalTime\n",
    "\n",
    "\n",
    "def plot_postResults(answer_y, pred_y, results, title=\"dtw\"):\n",
    "    fig = plt.figure(figsize=(18,12))\n",
    "    \n",
    "    Gnd_fig = fig.add_subplot(2,3,1)\n",
    "    Gnd_fig.set_title(\" Ground Truth \")\n",
    "    Gnd_fig.plot(answer_y, \"g\", label = \"Ground Truth\")\n",
    "    plt.legend()\n",
    "    \n",
    "    pred_fig = fig.add_subplot(2,3,2)\n",
    "    pred_fig.set_title(\" Prediction Result (Before DTW) \")\n",
    "    pred_fig.plot(pred_y, \"b\", label=\" pred_y (before DTW)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    dtw_fig = fig.add_subplot(2,3,3)\n",
    "    dtw_fig.set_title(\" PostProcessing Results (DTW) \")\n",
    "    dtw_fig.plot(results, \"r\",  label=\"dtw_result\")\n",
    "    plt.legend()\n",
    "    \n",
    "    gnd_pred_fig = fig.add_subplot(2,3,4)\n",
    "    gnd_pred_fig.set_title(\" Comparison: Ground Truth vs Prediction\")\n",
    "    gnd_pred_fig.plot(answer_y, \"g\", label=\"Ground Truth\") # linewidth=3\n",
    "    gnd_pred_fig.plot(pred_y, \"b--\", label=\" pred_y (before DTW)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    pred_dtw_fig = fig.add_subplot(2,3,5)\n",
    "    pred_dtw_fig.set_title(\" Comparison: Prediction vs DTW alignment\")\n",
    "    pred_dtw_fig.plot(pred_y, \"b--\", label=\" pred_y (before DTW)\") # linewidth=3\n",
    "    pred_dtw_fig.plot(results, \"r\", label=\" dtw_result\")\n",
    "    plt.legend()\n",
    "    \n",
    "    gnd_dtw_fig = fig.add_subplot(2,3,6)\n",
    "    gnd_dtw_fig.set_title(\" Comparison: Ground Truth vs DTW alignment \")\n",
    "    gnd_dtw_fig.plot(answer_y, \"g--\", label=\"Ground Truth\")\n",
    "    gnd_dtw_fig.plot(results, \"r\", label=\"dtw_result\")  # linewidth=3\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/DTW_Comparison_\" + title  + \".png\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_testdata(pelvis_csv, lookback_window = 8):\n",
    "    test_x = np.zeros((0, utils.lookback_window,  utils.numPelvis,1 ))\n",
    "    test_y = np.zeros((0,  utils.numActions)) \n",
    "   \n",
    "    timstamp_ms, lpfPelvis_x, lpfPelvis_y, lpfPelvis_z, oneHot_actionList  =  utils.getLPF_PelvisData(pelvis_csv)  # raw                 \n",
    "    pelvisData = np.array([lpfPelvis_x, lpfPelvis_y, lpfPelvis_z]).T  # (335,3) \n",
    "    actionData = np.array(oneHot_actionList)    # # (335, 5)\n",
    "                            \n",
    "    blockTime, blockPelvis, blockLable =  utils.sliding_window(timstamp_ms, pelvisData, actionData)\n",
    "    test_x = np.append(test_x, blockPelvis, axis = 0 )\n",
    "    test_y = np.append(test_y, blockLable, axis = 0 )    \n",
    "\n",
    "    return blockTime, test_x, test_y\n",
    "\n",
    "\n",
    "def barplot_splitResults(pred_subTask_frame, true_subTask_frame, pred_totalTime, true_totalTime, title= \"normal_20\"):\n",
    "    subTask_label = ['sit-to-stand', 'walk', 'turn', 'walk-back', 'sit-back']\n",
    "    actIndex = np.arange(len( [\"Pred\", \"Truth\"]))\n",
    "    color = ['b','g','r','c','y']\n",
    "    \n",
    "        \n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.title('TUG sub-Task segmentation Results\\n', loc='center')\n",
    "    plt.xlabel('frames')\n",
    "    plt.yticks(actIndex,  [\"Model Pred\", \"Ground Truth\"])\n",
    "    plt.text(10, 0.5, \"True totalTime: \" + str(np.round(true_totalTime,4)) + \"  ,   Pred totalTime: \" + str(np.round(pred_totalTime,4)), fontsize=13)\n",
    "    plt.text(10,-0.8, \"TotalTime Error (sec): \" + str(np.round(np.abs(true_totalTime-pred_totalTime),4)), fontsize=13, fontweight='bold')\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(color)):\n",
    "        rects = plt.barh(actIndex,  [pred_subTask_frame[i] , true_subTask_frame[i]] , color = color[i], left = [ np.sum(np.sum(pred_subTask_frame[:i])),  np.sum(np.sum(true_subTask_frame[:i]))] )\n",
    " \n",
    "        for j, rect in enumerate(rects):\n",
    "            if j is 0:  # --- pred\n",
    "                h1 = rect.get_height()\n",
    "                w1 = rect.get_width()\n",
    "                plt.text(rect.get_x() + rect.get_width() / 2., h1 / 1., \"%d\" % true_subTask_frame[i], color=\"k\", fontsize=12, fontweight=\"bold\")         \n",
    "\n",
    "            else:  # -- true \n",
    "                h2 = rect.get_height()\n",
    "                w2 = rect.get_width()\n",
    "                plt.text(rect.get_x() + w2/ 2., (h2-h1) / 1.,\"%d\" % pred_subTask_frame[i], color=\"k\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        \n",
    "\n",
    "    plt.legend(subTask_label,  bbox_to_anchor=([1, 1, 0, 0]))                         \n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+\"/BarGragh_FrameSplit_\" + title  + \".png\")\n",
    "                         \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-complaint",
   "metadata": {},
   "source": [
    "### 정상인 20대, 30대, 60대 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-metropolitan",
   "metadata": {},
   "source": [
    "test_normal_20 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm20/jek/02/Originact5_lpf_labeled_jek_02.csv\"\n",
    "test_normal_20_time, test_normal_20_x, test_normal_20_y_onehot = create_testdata(test_normal_20) \n",
    "test_normal_20_y = np.argmax(test_normal_20_y_onehot, axis=1).reshape(-1) # GND one-hot decode\n",
    "\n",
    "pred_normal_20_y_onehot = model.predict(test_normal_20_x) \n",
    "pred_normal_20_y = np.argmax(pred_normal_20_y_onehot, axis=1) #  # pred one-hot decode\n",
    "\n",
    "title = \"normal_20\"\n",
    "normal_20_alignment, normal_20_results, true_subTask_frame_20, true_totalTime_20, pred_subTask_frame_20, pred_totalTime_20 = post_process_DTW(test_normal_20_time, test_normal_20_y, pred_normal_20_y, answer_y, title = title)\n",
    "\n",
    "# --------------- DTW Result Visualization ------------------------ #\n",
    "normal_20_alignment.plot(type=\"threeway\")\n",
    "plot_postResults(test_normal_20_y, pred_normal_20_y, normal_20_results, title = title)\n",
    "norm_20_eval = model.evaluate(test_normal_20_x, test_normal_20_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_20.txt\",'w+t')\n",
    "print(\"[Before DTW] Before_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, pred_normal_20_y, target_names=target_names), file = report_dtw)\n",
    "print(\"------------------------------\\n\", file = report_dtw)\n",
    "print(\"[After DTW] After_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, normal_20_results, target_names=target_names), file = report_dtw)\n",
    "report_dtw.close()\n",
    "\n",
    "# # ----------- Visualize in cell\n",
    "print(\"[Before DTW] Before_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, pred_normal_20_y, target_names=target_names))\n",
    "print(\"------------------------------\\n\")\n",
    "print(\"[After DTW] After_DTW_Norm_20 \\n\", classification_report(test_normal_20_y, normal_20_results, target_names=target_names))\n",
    "\n",
    "\n",
    "print(\" [True]\", true_subTask_frame_20, true_totalTime_20,\"\\n [Pred]\", pred_subTask_frame_20, pred_totalTime_20) \n",
    "barplot_splitResults(pred_subTask_frame_20, true_subTask_frame_20,pred_totalTime_20, true_totalTime_20, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-sterling",
   "metadata": {},
   "source": [
    "test_normal_30 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm20/jdh/03/Originact5_lpf_labeled_jdh_03.csv\"\n",
    "test_normal_30_time, test_normal_30_x, test_normal_30_y_onehot = create_testdata(test_normal_30) \n",
    "test_normal_30_y = np.argmax(test_normal_30_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "pred_normal_30_onehot = model.predict(test_normal_30_x)\n",
    "pred_normal_30_y = np.argmax(pred_normal_30_onehot, axis=1)\n",
    "\n",
    "title = \"normal_30\"\n",
    "normal_30_alignment, normal_30_results,  true_subTask_frame_30, true_totalTime_30, pred_subTask_frame_30, pred_totalTime_30 = post_process_DTW(test_normal_30_time, test_normal_30_y, pred_normal_30_y, answer_y, title=title)\n",
    "\n",
    "# --------------- DTW Result Visualization ------------------------ #\n",
    "normal_30_alignment.plot(type=\"threeway\")\n",
    "plot_postResults(test_normal_30_y, pred_normal_30_y, normal_30_results, title=title)\n",
    "norm_30_eval = model.evaluate(test_normal_30_x, test_normal_30_y_onehot,batch_size= batch_size,verbose=2)\n",
    "\n",
    "report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_30.txt\",'w+t')\n",
    "print(\"[Before DTW] Before_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, pred_normal_30_y, target_names=target_names), file = report_dtw)\n",
    "print(\"------------------------------\\n\", file = report_dtw)\n",
    "print(\"[After DTW] After_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, normal_30_results, target_names=target_names), file = report_dtw)\n",
    "report_dtw.close()\n",
    "\n",
    "# # ----------- Visualize in cell\n",
    "print(\"[Before DTW] Before_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, pred_normal_30_y, target_names=target_names))\n",
    "print(\"------------------------------\\n\")\n",
    "print(\"[After DTW] After_DTW_Norm_30 \\n\", classification_report(test_normal_30_y, normal_30_results, target_names=target_names))\n",
    "\n",
    "print(\" [True]\", true_subTask_frame_30, true_totalTime_30,\"\\n [Pred]\", pred_subTask_frame_30, pred_totalTime_30) \n",
    "barplot_splitResults(pred_subTask_frame_30, true_subTask_frame_30, pred_totalTime_30,true_totalTime_30, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-boards",
   "metadata": {},
   "source": [
    "# test_normal_60 = \"/DockerProjects/Dataset/TUG/trainSet/HMM_saveResults_최윤정/0_sideView/2021_02_09 TUG/UMS_TUG/05/Originact5_lpf_labeled_UMS_TUG_05.csv\"\n",
    "test_normal_60 = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/norm60/LBL_TUG/05/Originact5_lpf_labeled_LBL_TUG_05.csv\"\n",
    "test_normal_60_time, test_normal_60_x, test_normal_60_y_onehot = create_testdata(test_normal_60) \n",
    "test_normal_60_y = np.argmax(test_normal_60_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "\n",
    "pred_normal_60_onehot = model.predict(test_normal_60_x)\n",
    "pred_normal_60_y = np.argmax(pred_normal_60_onehot, axis=1).reshape(-1) \n",
    "\n",
    "title = \"normal_60\"\n",
    "normal_60_alignment, normal_60_results, true_subTask_frame_60, true_totalTime_60, pred_subTask_frame_60, pred_totalTime_60 = post_process_DTW(test_normal_60_time, test_normal_60_y, pred_normal_60_y, answer_y, title=title)\n",
    "\n",
    "\n",
    "# --------------- DTW Result Visualization ------------------------ #\n",
    "normal_60_alignment.plot(type=\"threeway\")\n",
    "plot_postResults(test_normal_60_y, pred_normal_60_y, normal_60_results, title=title)\n",
    "norm_60_eval = model.evaluate(test_normal_60_x, test_normal_60_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_Norm_60.txt\",'w+t')\n",
    "print(\"[Before DTW] Before DTW Norm 60 \\n\", classification_report(test_normal_60_y, pred_normal_60_y, target_names=target_names), file = report_dtw)\n",
    "print(\"------------------------------\\n\",file = report_dtw)\n",
    "print(\"[After DTW] After_DTW_Norm_60 \\n\", classification_report(test_normal_60_y, normal_60_results, target_names=target_names), file =   report_dtw)\n",
    "report_dtw.close()\n",
    "\n",
    "# ----------- Visualize in cell\n",
    "print(\"[Before DTW] Before DTW Norm 60 \\n\", classification_report(test_normal_60_y, pred_normal_60_y, target_names=target_names))\n",
    "print(\"------------------------------\\n\")\n",
    "print(\"[After DTW] After_DTW_Norm_60 \\n\", classification_report(test_normal_60_y, normal_60_results, target_names=target_names))\n",
    "\n",
    "print(\" [True]\", true_subTask_frame_60, true_totalTime_60,\"\\n [Pred]\", pred_subTask_frame_60, pred_totalTime_60) \n",
    "barplot_splitResults(pred_subTask_frame_60, true_subTask_frame_60, pred_totalTime_60, true_totalTime_60, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-dealer",
   "metadata": {},
   "source": [
    "###  Stroke -- 보조필요 없는 수준의 편마비, 케인/지팡이 사용,  모방 보행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-dutch",
   "metadata": {},
   "source": [
    "test_stroke_1 =  \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/stroke/JDW_tug/02/Originact5_lpf_labeled_JDW_tug_02.csv\"\n",
    "test_normal_stroke_time, test_stroke_1_x, test_stroke_1_y_onehot = create_testdata(test_stroke_1)  \n",
    "test_stroke_1_y = np.argmax(test_stroke_1_y_onehot, axis=1).reshape(-1) # one-hot decode\n",
    "\n",
    "pred_stroke_1_one_hot = model.predict(test_stroke_1_x)\n",
    "pred_stroke_1_y = np.argmax(pred_stroke_1_one_hot, axis=1)\n",
    "\n",
    "title=\"stroke_1\"\n",
    "stroke_1_alignment, stroke_1_results, true_subTask_frame_stroke, true_totalTime_stroke, pred_subTask_frame_stroke, pred_totalTime_stroke = post_process_DTW(test_normal_stroke_time, test_stroke_1_y, pred_stroke_1_y, answer_y,  title=title)\n",
    "\n",
    "\n",
    "# --------------- DTW Result Visualization ------------------------ #\n",
    "stroke_1_alignment.plot(type=\"threeway\")\n",
    "plot_postResults(test_stroke_1_y, pred_stroke_1_y, stroke_1_results, title=title)\n",
    "stroke_1_eval = model.evaluate(test_stroke_1_x, test_stroke_1_y_onehot, batch_size= batch_size,verbose=2)\n",
    "\n",
    "report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_stroke_1.txt\",'w+t')\n",
    "print(\"[Before DTW] Before DTW stroke_1 \\n\", classification_report(test_stroke_1_y, pred_stroke_1_y, target_names=target_names), file = report_dtw)\n",
    "print(\"------------------------------\\n\", file = report_dtw)\n",
    "print(\"[After DTW] After DTW stroke_1 \\n\", classification_report(test_stroke_1_y, stroke_1_results, target_names=target_names), file = report_dtw)\n",
    "report_dtw.close()\n",
    "\n",
    "# # ----------- Visualize in cell\n",
    "print(\"[Before DTW] Before DTW stroke_1 \\n\", classification_report(test_stroke_1_y, pred_stroke_1_y, target_names=target_names))\n",
    "print(\"------------------------------\\n\")\n",
    "print(\"[After DTW] After DTW stroke_1 \\n\", classification_report(test_stroke_1_y, stroke_1_results, target_names=target_names))\n",
    "\n",
    "print(\" [True]\", true_subTask_frame_stroke, true_totalTime_stroke,\"\\n [Pred]\", pred_subTask_frame_stroke, pred_totalTime_stroke) \n",
    "barplot_splitResults(pred_subTask_frame_stroke, true_subTask_frame_stroke, pred_totalTime_stroke, true_totalTime_stroke, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-plymouth",
   "metadata": {},
   "source": [
    "test_stroke_tool = \"/DockerProjects/Dataset/TUG/trainSet/HMMpaper/HMM_saveResults_illness_CYJ/0_sideView/stroke/KNG_TUG/03/Originact5_lpf_labeled_KNG_TUG_03.csv\"\n",
    "test_stroke_tool_time, test_stroke_tool_x, test_stroke_tool_y_onehot = create_testdata(test_stroke_tool) \n",
    "test_stroke_tool_y = np.argmax(test_stroke_tool_y_onehot, axis=1).reshape(-1) #one-hot decode\n",
    "\n",
    "pred_stroke_tool_onehot = model.predict(test_stroke_tool_x)\n",
    "pred_stroke_tool_y = np.argmax(pred_stroke_tool_onehot, axis=1)\n",
    "\n",
    "title = \"stroke_tool\"\n",
    "stroke_tool_alignment, stroke_tool_results, true_subTask_frame_stroke_tool, true_totalTime_stroke_tool, pred_subTask_frame_stroke_tool, pred_totalTime_stroke_tool = post_process_DTW(test_stroke_tool_time, test_stroke_tool_y, pred_stroke_tool_y, answer_y, title=title)\n",
    "\n",
    "\n",
    "# --------------- DTW Result Visualization ------------------------ #\n",
    "stroke_tool_alignment.plot(type=\"threeway\")\n",
    "plot_postResults(test_stroke_tool_y, pred_stroke_tool_y, stroke_tool_results, title=title)\n",
    "stroke_tool_eval = model.evaluate(test_stroke_tool_x, test_stroke_tool_y_onehot,batch_size= batch_size,verbose=2)\n",
    "\n",
    "report_dtw = open(MODEL_SAVE_FOLDER_PATH + \"/Comp_DTW_stroke_tool.txt\",'w+t')\n",
    "print(\"[Before DTW] Before DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, pred_stroke_tool_y, target_names=target_names), file = report_dtw)\n",
    "print(\"------------------------------\\n \", file = report_dtw)\n",
    "print(\"[After DTW] After DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, stroke_tool_results, target_names=target_names), file = report_dtw)\n",
    "report_dtw.close()\n",
    "\n",
    "# # ----------- Visualize in cell\n",
    "print(\"[Before DTW] Before DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, pred_stroke_tool_y, target_names=target_names))\n",
    "print(\"------------------------------\\n \")\n",
    "print(\"[After DTW] After DTW stroke_tool \\n\", classification_report(test_stroke_tool_y, stroke_tool_results, target_names=target_names))\n",
    "\n",
    "print(\" [True]\", true_subTask_frame_stroke_tool, true_totalTime_stroke_tool,\"\\n [Pred]\", pred_subTask_frame_stroke_tool, pred_totalTime_stroke_tool) \n",
    "barplot_splitResults(pred_subTask_frame_stroke_tool, true_subTask_frame_stroke_tool, pred_totalTime_stroke_tool, true_totalTime_stroke_tool, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-canada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Home Dir.] Current path /DockerProjects/walkCAM/tug/0_temporalCNN_wLabel\n",
      " Current workaing path [dataset]  /DockerProjects/Dataset/TUG/trainSet/2021_01_24_saveResults_최윤정/0_sideView\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold  # 라벨 x_data, y_data 분포를 유지 (함께 fold)\n",
    "import tensorflow as tf \n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(7)\n",
    "np.set_printoptions(precision=4, suppress=True)  # 과학적 표기 대신 소숫점 자리 4자리까지 표현\n",
    "\n",
    "rootDir = \"/DockerProjects/Dataset/TUG/trainSet\"\n",
    "expertFolder = \"/\" + \"2021_01_24_saveResults_최윤정\"  # --- 변경 할 부분\n",
    "viewFolder = \"/\" + \"0_sideView\"\n",
    "\n",
    "print(\" [Home Dir.] Current path\", os.getcwd())  \n",
    "\n",
    "datasetDir = rootDir + expertFolder + viewFolder\n",
    "os.chdir(datasetDir) ; print(\" Current workaing path [dataset] \", os.getcwd())  # -- Dataset 있는 곳으로 경로 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjectList(datasetDir, shuffle = False):      \n",
    "    expDates = next(os.walk(datasetDir))[1]   #['2020_11_03', '2020_11_20_v1', '2020_11_20_v2', '2020_11_23', '2020_12_09', '2020_12_30']\n",
    "    subject_number = 0\n",
    "    subjects_list = []\n",
    "    \n",
    "    for dateFolder in expDates:\n",
    "        dateDir = os.path.join(datasetDir, dateFolder)   # D:/바탕화면/Dataset/TUG/trainSet/2021_01_24_saveResults_최윤정/0_sideView\\2020_11_03\n",
    "        tmpSubjects = next(os.walk(dateDir))[1]\n",
    "        subjects_list.append(tmpSubjects)  \n",
    "        subject_number += len(tmpSubjects)\n",
    "\n",
    "    # ------ 2D -> 1D [[sub1, sub2], [sub3, sub4]] -> [sub1, sub2, sub3, sub4]\n",
    "    subjects = []\n",
    "    for eachSub in subjects_list:\n",
    "        subjects += eachSub\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(subjects)\n",
    "        \n",
    "    print(\" 1D sub list: \" , subjects)\n",
    "    return subjects, subject_number\n",
    "\n",
    "def fold_trainNames(subList, Kfold_num = 5):\n",
    "    assert (Kfold_num is not 0), print(\"check kfold_num. should not be zero\")\n",
    "    Fold_results =[]\n",
    "    kfold = KFold(n_splits=  Kfold_num)\n",
    "    \n",
    "    for trainIdx, testIdx in kfold.split(subList):\n",
    "        Fold_results.append([trainIdx, testIdx])      \n",
    "        \n",
    "    train_Fold= []\n",
    "    test_Fold = []\n",
    "        \n",
    "    # bind fold pair\n",
    "    for i in range(Kfold_num):\n",
    "        tmp_train = []\n",
    "        tmp_test = []\n",
    "        for subIdx in range(len(subList)): \n",
    "            if subIdx in Fold_results[i][0]:  # \n",
    "                tmp_train.append(subList[subIdx])\n",
    "                    \n",
    "            if subIdx in Fold_results[i][1]:  # -- K-fold test\n",
    "                tmp_test.append(subList[subIdx])\n",
    "          \n",
    "        train_Fold.append(tmp_train)\n",
    "        test_Fold.append(tmp_test)\n",
    "        print(\"\\t [process]\", i, \"-fold:\", train_Fold[i], \"\\n\\t\\t\\t   ,\", test_Fold[i])\n",
    "    print(\"\\n [results]  trainSet: \", len(train_Fold[0]) , \" 명 , testSet: \", len(test_Fold[0]), \" 명\" )\n",
    "    \n",
    "    return train_Fold, test_Fold\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train/Test Set -- subject 기준으로 분할   \n",
    "##### (Kfold_num = 0 이면 직접 분할, 5 등 숫자면 5-fold dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Key    \n",
    "    https://github.com/ashishpatel26/tcn-keras-Examples  \n",
    "\n",
    "sub  \n",
    "      https://www.programmersought.com/article/13674618779/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1D sub list:  ['bys_tug', 'cbd_tug', 'cyj_tug', 'pss', 'yjh', 'lhs', 'kyh', 'nhs', 'kw', 'NHJ_60', 'pjh', 'rjh', 'ryu', 'JDW_tug', 'PHE_TUG', 'YSJ_TUG', 'CYJ_TUG', 'BYS_TUG', 'jdh', 'cbd', 'kch', 'bys', 'jek', 'cyj', 'cyj2']\n",
      "\n",
      "\n",
      " No-fold --- (arbitrary) \t [results] \n",
      "\t Train:  ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj'] \n",
      "\t Valid:  ['cyj2', 'jdh', 'jek', 'lhs'] \n",
      "\t Test:  ['kch', 'ryu', 'NHJ_60']\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.6\n",
    "valid_ratio = 0.2\n",
    "\n",
    "arbitrarySplit = True\n",
    "Kfold = False\n",
    "Kfold_num = 5 # if kfold_num = 0 (arbitrary train/test set will be used)s\n",
    "\n",
    "\n",
    "numFeature = 3\n",
    "numActions = 5\n",
    "dataset_columns = numFeature + numActions\n",
    "\n",
    "subList, numOfsubjects = get_subjectList(datasetDir, shuffle = False) ; print(\"\\n\")\n",
    "\n",
    "if arbitrarySplit is not True:\n",
    "    if Kfold:\n",
    "        print(\" K-fold : \", Kfold_num)\n",
    "        trainFold, testFold = fold_trainNames(subList = subList, Kfold_num = Kfold_num)\n",
    "\n",
    "        selected_foldNum = 0 \n",
    "\n",
    "        train_subjects = trainFold[selected_foldNum]\n",
    "        test_subjects = testFold[selected_foldNum]\n",
    "        print(\"\\t ------>  Selected Fold num: %d \\n   Train Subjects: %s\\n   Test Subjects: %s\"%(selected_foldNum, train_subjects, test_subjects))\n",
    "    else:\n",
    "\n",
    "        trainSub_number = np.round( (numOfsubjects*train_ratio), 0).astype(int)\n",
    "        validSub_number = np.round( (numOfsubjects*valid_ratio), 0).astype(int)\n",
    "        testSub_number = numOfsubjects - trainSub_number - validSub_number\n",
    "\n",
    "        train_subjects = subList[ :trainSub_number]\n",
    "        valid_subjects = subList[ trainSub_number : (trainSub_number+validSub_number)]\n",
    "        test_subjects = subList[ (trainSub_number+validSub_number): ]\n",
    "        print(\"   No-fold \\t [results] \\n\\t Train: \" ,train_subjects, \"\\n\\t Valid: \", valid_subjects, \"\\n\\t Test: \", test_subjects)\n",
    "        print(\"\\n\\t--> # of train  : \" + str(trainSub_number) +\" 명 ,  # of valid : \"+ str(validSub_number) + \" 명 ,  # of test :\" + str(testSub_number) + \"명\")\n",
    "\n",
    "else:\n",
    "    #         normal_subjects = ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'lhs', 'NHJ_60', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj', 'cyj2', 'jdh', 'jek', 'kch', 'ryu'] \n",
    "#         patients_subjects = ['JDW_tug']\n",
    "#         copy_patients_subjects = ['BYS_TUG', 'CYJ_TUG', 'PHE_TUG', 'YSJ_TUG'] \n",
    "\n",
    "    train_subjects = ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj', ] \n",
    "    valid_subjects = ['cyj2', 'jdh', 'jek', 'lhs' ] \n",
    "    test_subjects =  ['kch', 'ryu', 'NHJ_60']\n",
    "    print(\" No-fold --- (arbitrary) \\t [results] \\n\\t Train: \" ,train_subjects, \"\\n\\t Valid: \", valid_subjects, \"\\n\\t Test: \", test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPelvisData(csvFile):\n",
    "    rawData = np.loadtxt(csvFile, delimiter=\",\")\n",
    "    timestamp = rawData[:,0]\n",
    "    pelvis_x = rawData[:,1]\n",
    "    pelvis_y = rawData[:,2]\n",
    "    pelvis_z = rawData[:,3]\n",
    "    actionList = rawData[:,4:9]\n",
    "    return timestamp, pelvis_x, pelvis_y, pelvis_z, actionList\n",
    "\n",
    "def sliding_window(timestamp, data_x, data_y, lookback_window=3):\n",
    "    time = []\n",
    "    x = []\n",
    "    y = []\n",
    "    enc = MinMaxScaler(feature_range = (0, 1))\n",
    "    enc_y = enc.fit_transform(data_y)\n",
    "    for i in range(lookback_window, len(data_x)+1):\n",
    "        time.append(timestamp[i-1])\n",
    "        x.append(data_x[i - lookback_window:i])\n",
    "        y.append(data_y[i-1])\n",
    "  \n",
    "    x = np.array(x)\n",
    "    x = x.reshape(-1, lookback_window, numFeature, 1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    time = np.array(time)\n",
    "   \n",
    "    return time, x, y, enc\n",
    "\n",
    "\n",
    "def create_dataset(train_subjects, test_subjects, lookback_window = 3):\n",
    "    trainSet = None\n",
    "    validSet = None\n",
    "    testSet = None\n",
    "    \n",
    "    for dirpath, foldername, files in sorted(os.walk(datasetDir)):\n",
    "        for filename in files:\n",
    "            if \"lpf_\" in filename:\n",
    "                subname = dirpath.split(\"/\")[-2] \n",
    "                csvFile = os.path.join(dirpath, filename)\n",
    "                          \n",
    "                timestamp, pelvis_x, pelvis_y, pelvis_z, actionList = getPelvisData(csvFile) \n",
    "                \n",
    "                pelvisData = np.array([pelvis_x, pelvis_y, pelvis_z]).T  # (317,3)\n",
    "                actionData = np.array(actionList)    # # (335, 5)\n",
    "    \n",
    "                blockTime, blockPelvis, blockLable, enc = sliding_window(timestamp, data_x = pelvisData, data_y = actionData, lookback_window=lookback_window) # pelvis - (327, 8, 3)\n",
    "                # bind as a tensor\n",
    "                dataset = tf.data.Dataset.from_tensor_slices( (blockPelvis.astype('float32'), blockLable.astype('float32')) )\n",
    "                \n",
    "                if subname in train_subjects:\n",
    "                    if trainSet is None:\n",
    "                        trainSet = dataset\n",
    "                    else:\n",
    "                        trainSet = trainSet.concatenate(dataset)\n",
    "                        \n",
    "                elif subname in test_subjects:\n",
    "                    if testSet is None:\n",
    "                        testSet = dataset\n",
    "                    else:\n",
    "                        testSet = testSet.concatenate(dataset)\n",
    "                else:\n",
    "                    if validSet is None:\n",
    "                        validSet = dataset\n",
    "                    else:\n",
    "                        validSet = validSet.concatenate(dataset)       \n",
    "            \n",
    "    return trainSet, validSet, testSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0763 0.7799 0.4384 0.7235 0.978  0.5385 0.5011 0.0721] \n",
      " [0 1 2 3 4 5 6 7] \n",
      "\n",
      "(TensorSpec(shape=(3,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        print(\"{:20s}: {}\".format('Label',label.numpy()))\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data_x = np.random.rand(8)\n",
    "data_y = np.array(range(8))\n",
    "\n",
    "print(data_x,\"\\n\", data_y,\"\\n\")\n",
    "x = []\n",
    "y = []\n",
    "lookback_window = 3\n",
    "for i in range(lookback_window, len(data_x)+1):\n",
    "    x.append(data_x[i - lookback_window:i])    \n",
    "    y.append(data_y[i-1])\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices( (x.astype('float32'), y.astype('float32')) )\n",
    "\n",
    "# for element in dataset:\n",
    "#     print(element)\n",
    "print(dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 3, 1), dtype=tf.float32, name=None), TensorSpec(shape=(5,), dtype=tf.float32, name=None))\n",
      "(TensorSpec(shape=(256, 8, 3, 1), dtype=tf.float32, name=None), TensorSpec(shape=(256, 5), dtype=tf.float32, name=None))\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "lookback_window = 8\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "learning_rate = 0.00001\n",
    "trainSet, validSet, testSet = create_dataset(train_subjects, test_subjects, lookback_window = lookback_window)\n",
    "trainSet_shuffled = trainSet.shuffle(10000).batch(batch_size, drop_remainder = True)\n",
    "\n",
    "print(trainSet.element_spec)\n",
    "print(trainSet_shuffled.element_spec)\n",
    "\n",
    "print(len(trainSet_shuffled))\n",
    "# cnt =0\n",
    "# print(len(trainSet))\n",
    "# print(len(trainSet_shuffled))\n",
    "# for element in trainSet:\n",
    "#     print(element[0], element[1])\n",
    "#     cnt+=1\n",
    "#     if cnt == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Input, Activation, Flatten, Dense, Conv2D\n",
    "from keras.layers import BatchNormalization, add , Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DockerProjects/Dataset/TUG/trainSet//2021_01_24_saveResults_최윤정/model_TCN/\n"
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = rootDir + \"/\"+ expertFolder +\"/model_TCN/\"\n",
    "\n",
    "print(MODEL_SAVE_FOLDER_PATH)\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "model_path = MODEL_SAVE_FOLDER_PATH + \"WinSize- \"+ str(lookback_window) +\"TUG-\" + \"{epoch:02d} ---- {val_loss: .4f}.hdf5\"\n",
    "\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c8b396d99ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "def custom_loss(y_actual,y_pred):\n",
    "    y_ = kb.sigmoid(y_pred)\n",
    "    custom_loss = constant * y_actual * kb.log(y_) + (1- y_actual) * kb.log(1 - y_)\n",
    "    return -tf.reduce_mean(custom_loss)\n",
    "\n",
    "class RNN(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.lstm1 = LSTM(15, return_sequences=True)\n",
    "        self.lstm2 = LSTM(10, return_sequences=True)\n",
    "        self.lastLayer = TimeDistributed(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.lastLayer(x)\n",
    "        print('rnn', x.shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    " #Residual block :: https://roadcom.tistory.com/95\n",
    "def ResBlock(x,filters,kernel_size,dilation_rate):\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate,activation='relu')(x) #first convolution\n",
    "    r=Conv2D(filters,kernel_size=kernel_size,padding='same',dilation_rate=dilation_rate)(r) #Second convolution\n",
    "    if x.shape[-1]==filters:\n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 일치할 경우 단순 add 연산만 진행하는 블록 = identity block\n",
    "        shortcut = x  # identity block \n",
    "    else: \n",
    "        # Shortcut 의 channel 과 main path 의 channel 이 다를 경우 shortcut path 를 적절히 변환\n",
    "        # 즉, projection 을 통해 channel 을 맞춰주는 작업이(projection shortcut) 추가되기에 이를 convolution block 이라함\n",
    "        shortcut=Conv2D(filters,kernel_size=kernel_size,padding='same')(x) \n",
    "    o=add([r,shortcut])\n",
    "    o=Activation('relu')(o) \n",
    "    return o\n",
    " \n",
    " #Sequence Model\n",
    "def TCN(optimizer, class_weight):\n",
    "    kernel_size = (3,3)\n",
    "    input_shape =  (lookback_window, 3, 1) # (3, 8, 1) = (feature, sliding_window, 1)\n",
    "    \n",
    "    inputs=Input(shape=input_shape)\n",
    "    \n",
    "    x=ResBlock(x = inputs,filters=32,kernel_size=kernel_size,dilation_rate=1)\n",
    "    x = Dropout(0.2) (x)\n",
    "    x=ResBlock(x,filters=32,kernel_size=kernel_size,dilation_rate=2)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x=ResBlock(x,filters=16,kernel_size=kernel_size,dilation_rate=4)\n",
    "    \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(5, activation='softmax')(x)\n",
    "    model=Model(inputs=inputs,outputs=x)\n",
    "         \n",
    "    model.summary()\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-57a7f50707dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# (feature, sliding_window, 1) ...TCN,,,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3.298\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6.929\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2.052\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.427\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# trainSet  trainSet_shuffled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "    \n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "# (feature, sliding_window, 1) ...TCN,,,  \n",
    "class_weight = {0:3.298, 1:1.0, 2:6.929, 3:2.052, 4:1.427}\n",
    "model = TCN(optimizer, class_weight)\n",
    "# trainSet  trainSet_shuffled\n",
    "# hist = model.fit(trainSet_shuffled, batch_size= batch_size, epochs=epochs,verbose=2, validation_data= validSet,  callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "# eval_result = model.evaluate(trainSet_shuffled,batch_size= batch_size,verbose=2)    \n",
    "hist = model.fit(trainSet, epochs=epochs,verbose=2, validation_data= validSet,  callbacks=[cb_checkpoint, cb_early_stopping], class_weight=class_weight)\n",
    "model.fit(dataset_shuffled, epochs=epoch)\n",
    "# eval_result = model.evaluate(testSet,verbose=2)    \n",
    " \n",
    "\n",
    "print('test_loss:',eval_result[0],'- test_acc:',eval_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "\n",
    "y_pred_onehot = np.argmax(y_pred, axis=1)\n",
    "print(y_pred[200] ,\"===> onehot: \", y_pred_onehot[200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix, Precision, Recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_argmax = np.argmax(y_test ,axis=1) \n",
    "y_pred_argmax = np.argmax(y_pred ,axis=1)\n",
    "print(y_test_argmax.shape, y_pred_argmax.shape)\n",
    "\n",
    "def action_frames(y_pred_argmax, ratio_display = False):\n",
    "    action_label =  {\"sit\": 0, \"sit-stand\": 1,  \"walk\": 2, \"turn\": 3, \"stand-sit\": 4}\n",
    "    action_cnt =  {\"total_frames\": 0,\"sit\": 0, \"sit-stand\": 0,  \"walk\": 0, \"turn\": 0, \"stand-sit\": 0}\n",
    "\n",
    "    for i in range(len(y_pred_argmax)):\n",
    "        action_cnt[\"total_frames\"] +=1\n",
    "        if y_pred_argmax[i] == action_label[\"sit\"]:\n",
    "            action_cnt[\"sit\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"sit-stand\"]:\n",
    "            action_cnt[\"sit-stand\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"walk\"]:\n",
    "            action_cnt[\"walk\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"turn\"]:\n",
    "            action_cnt[\"turn\"] +=1\n",
    "            \n",
    "        elif y_pred_argmax[i] == action_label[\"stand-sit\"]:\n",
    "            action_cnt[\"stand-sit\"] +=1\n",
    "        \n",
    "    if ratio_display:     \n",
    "        print(\"[ Action Ratio ]\")\n",
    "        print(\"\\t [ 0 - Sit] ratio of sit: \",  action_cnt[\"sit\"]/action_cnt[\"total_frames\"] )\n",
    "        print(\"\\t [ 1 - sit-stand] ratio of sit-stand: \",  action_cnt[\"sit-stand\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 2 - walk] ratio of walk: \",  action_cnt[\"walk\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 3 - turn] ratio of turn: \",  action_cnt[\"turn\"]/action_cnt[\"total_frames\"])\n",
    "        print(\"\\t [ 4 - stand-sit] ratio of stand-sit: \",  action_cnt[\"stand-sit\"]/action_cnt[\"total_frames\"])\n",
    "\n",
    "\n",
    "    \n",
    "    return action_cnt\n",
    "    \n",
    "y_pred_frames = action_frames(y_pred_argmax, ratio_display= True)\n",
    "y_test_frames = action_frames(y_test_argmax,  ratio_display= True)   \n",
    "\n",
    "print(\"--->  # of Each Action Frames  \\n\\t y_pred: {0}, \\n\\t y_test: {1}\".format(y_pred_frames, y_test_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "kappa = cohen_kappa_score(y_test_argmax, y_pred_argmax)\n",
    "print('Cohens kappa: %f' % kappa)\n",
    "\n",
    "print(classification_report(y_test_argmax, y_pred_argmax))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_argmax, y_pred_argmax)\n",
    "print(\"\\n---> Confusion Matrix \\n\" ,conf_matrix) # sit, sit-stand, walking, turning, stand-sit\n",
    "\n",
    "plt.figure(figsize = (15,7) )\n",
    "sns.heatmap(conf_matrix, annot=True)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

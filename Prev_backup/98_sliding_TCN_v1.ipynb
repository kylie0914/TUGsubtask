{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Home Dir.] Current path D:\\바탕화면\\conda\\tug\\own\\tug-main\\0_temporalCNN_wLabel\n",
      " Current workaing path [dataset]  D:\\바탕화면\\Dataset\\TUG\\trainSet\\2021_01_24_saveResults_최윤정\\0_sideView\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold  # 라벨 x_data, y_data 분포를 유지 (함께 fold)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(7)\n",
    "np.set_printoptions(precision=4, suppress=True)  # 과학적 표기 대신 소숫점 자리 4자리까지 표현\n",
    "\n",
    "rootDir = \"D:/바탕화면/Dataset/TUG/trainSet\"\n",
    "expertFolder = \"/\" + \"2021_01_24_saveResults_최윤정\"  # --- 변경 할 부분\n",
    "viewFolder = \"/\" + \"0_sideView\"\n",
    "\n",
    "print(\" [Home Dir.] Current path\", os.getcwd())  \n",
    "\n",
    "datasetDir = rootDir + expertFolder + viewFolder\n",
    "os.chdir(datasetDir) ; print(\" Current workaing path [dataset] \", os.getcwd())  # -- Dataset 있는 곳으로 경로 변경 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Set -- subject 기준으로 분할   \n",
    "##### (Kfold_num = 0 이면 직접 분할, 5 등 숫자면 5-fold dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainData_split(datasetDir,train_ratio = 0.8, shuffle = False,  Kfold_num = 5):      \n",
    "    expDates = next(os.walk(datasetDir))[1]   #['2020_11_03', '2020_11_20_v1', '2020_11_20_v2', '2020_11_23', '2020_12_09', '2020_12_30']\n",
    "    \n",
    "    subject_number = 0\n",
    "    subjects_list = []\n",
    "    \n",
    "    for dateFolder in expDates:\n",
    "        dateDir = os.path.join(datasetDir, dateFolder)   # D:/바탕화면/Dataset/TUG/trainSet/2021_01_24_saveResults_최윤정/0_sideView\\2020_11_03\n",
    "        tmpSubjects = next(os.walk(dateDir))[1]\n",
    "        subjects_list.append(tmpSubjects)  \n",
    "        subject_number += len(tmpSubjects)\n",
    "\n",
    "    # ------ 2D -> 1D [[sub1, sub2], [sub3, sub4]] -> [sub1, sub2, sub3, sub4]\n",
    "    subjects = []\n",
    "    for eachSub in subjects_list:\n",
    "        subjects += eachSub\n",
    "    print(\" [before shuffle] 1D sub list: \" , subjects)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(subjects)\n",
    "        print(\" [After shuffle] 1D sub list: \" , subjects)\n",
    "        \n",
    "    # ---- Split train/test subjects (for arbitrary split)\n",
    "    trainSub_number = np.round( (len(subjects)*train_ratio), 0).astype(int)\n",
    "    testSub_number = subject_number - trainSub_number\n",
    "    \n",
    "#     # --- K-fold cross validation\n",
    "    \n",
    "\n",
    "    if Kfold_num is not 0:\n",
    "        Fold_results =[]\n",
    "        print(\" [K-Fold] K = \", Kfold_num)\n",
    "        kfold = KFold(n_splits=  Kfold_num)\n",
    "        for trainIdx, testIdx in kfold.split(subjects):\n",
    "            Fold_results.append([trainIdx, testIdx])        \n",
    "        train_Fold= []\n",
    "        test_Fold = []\n",
    "        \n",
    "        for i in range(Kfold_num):\n",
    "            tmp_train = []\n",
    "            tmp_test = []\n",
    "            for subIdx in range(len(subjects)): \n",
    "                if subIdx in Fold_results[i][0]:  # \n",
    "                    tmp_train.append(subjects[subIdx])\n",
    "                    \n",
    "                if subIdx in Fold_results[i][1]:  # -- K-fold test\n",
    "                    tmp_test.append(subjects[subIdx])\n",
    "                    \n",
    "            train_Fold.append(tmp_train)\n",
    "            test_Fold.append(tmp_test)\n",
    "            print(\"\\t [process]\", i, \"-fold:\", train_Fold[i], \"\\n\\t\\t\\t   ,\", test_Fold[i])\n",
    "        print(\"\\n [results]  trainSet: \", len(train_Fold[0]) , \" 명 , testSet: \", len(test_Fold[0]), \" 명\" )\n",
    "        return train_Fold, test_Fold\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print(\"--------- no fold ----\")\n",
    "        train_subjects = subjects[ :trainSub_number]\n",
    "        test_subjects = subjects[trainSub_number: ]\n",
    "        print(\"[results]  \" ,train_subjects, \"\\n\", test_subjects)\n",
    "        print(\"\\t--> # of train sub: \" + str(trainSub_number) +\"명  ,  # of test subjects: \" + str(testSub_number) + \"명\")\n",
    "        return train_subjects, test_subjects \n",
    "\n",
    "\n",
    "def getPelvisData(csvFile):\n",
    "    rawData = np.loadtxt(csvFile, delimiter=\",\")\n",
    "    timestamp = rawData[:,0]\n",
    "    pelvis_x = rawData[:,1]\n",
    "    pelvis_y = rawData[:,2]\n",
    "    pelvis_z = rawData[:,3]\n",
    "    actionList = rawData[:,4:9]\n",
    "    return timestamp, pelvis_x, pelvis_y, pelvis_z, actionList\n",
    "\n",
    "def create_dataset(train_subjects):\n",
    "    train_x = np.zeros((0, numFeature))\n",
    "    train_y = np.zeros((0, numActions))\n",
    "    test_x = np.zeros((0, numFeature))\n",
    "    test_y = np.zeros((0, numActions))    \n",
    "    \n",
    "    trainSet = None\n",
    "    testSet = None\n",
    "    \n",
    "    for dirpath, foldername, files in os.walk(datasetDir):\n",
    "        for filename in files:\n",
    "            if \"lpf_\" in filename:\n",
    "                subname = dirpath.split(\"\\\\\")[2] \n",
    "                csvFile = os.path.join(dirpath, filename)\n",
    "                timestamp, pelvis_x, pelvis_y, pelvis_z, actionList = getPelvisData(csvFile) \n",
    "                \n",
    "                pelvisData = np.array([pelvis_x, pelvis_y, pelvis_z]).T\n",
    "                actionData = np.array(actionList)\n",
    "                #dataset = tf.data.Dataset.from_tensor_slices( (pelvisData.astype('float32'), actionData.astype('float32')) ) \n",
    "                dataset = np.zeros( (len(pelvisData), dataset_columns) ) \n",
    "                \n",
    "                dataset[:, :-(numActions)] = pelvisData      # time , pelvis  넣음  (317,4) -- raw data \n",
    "                dataset[:, -(numActions): ] = actionData     # (335, 5) one hot encoding 수행한 actionsList\n",
    "                \n",
    "\n",
    "                if subname in train_subjects:\n",
    "                    train_x = np.append(train_x, pelvisData, axis = 0 )\n",
    "                    train_y = np.append(train_y, actionData, axis = 0 )\n",
    "\n",
    "                else:\n",
    "                    test_x = np.append(test_x, pelvisData, axis = 0 )\n",
    "                    test_y = np.append(test_y, actionData, axis = 0 )    \n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [before shuffle] 1D sub list:  ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'lhs', 'NHJ_60', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj', 'cyj2', 'jdh', 'jek', 'kch', 'ryu', 'JDW_tug', 'BYS_TUG', 'CYJ_TUG', 'PHE_TUG', 'YSJ_TUG']\n",
      "--------- no fold ----\n",
      "[results]   ['bys_tug', 'cbd_tug', 'cyj_tug', 'kw', 'kyh', 'lhs', 'NHJ_60', 'nhs', 'pjh', 'pss', 'rjh', 'yjh', 'bys', 'cbd', 'cyj', 'cyj2', 'jdh', 'jek'] \n",
      " ['kch', 'ryu', 'JDW_tug', 'BYS_TUG', 'CYJ_TUG', 'PHE_TUG', 'YSJ_TUG']\n",
      "\t--> # of train sub: 18명  ,  # of test subjects: 7명\n",
      "\n",
      " Train Shape X , Y: (69605, 3) , (69605, 5) \t Test Shape X , Y: (36119, 3) , (36119, 5)\n"
     ]
    }
   ],
   "source": [
    "# ---------------  1. subject 별로 dataset 분할 (K-fold 정함) ------------------------------------------------------#\n",
    "Kfold_num = 0 # if kfold_num = 0 (arbitrary train/test set will be used)\n",
    "train_Fold, test_Fold = trainData_split(datasetDir,train_ratio = 0.7, shuffle = False, Kfold_num = Kfold_num)  \n",
    "\n",
    "\n",
    "numFeature = 3\n",
    "numActions = 5\n",
    "dataset_columns = numFeature + numActions\n",
    "\n",
    "# ---------------  2. dataset 생성  train_x, train_y, test_x, test_y  (k-fold 면 앞에 KFold_ 붙음) -----------------#\n",
    "if Kfold_num == 0:  \n",
    "    train_x, train_y, test_x, test_y = create_dataset(train_Fold)\n",
    "    print(\"\\n Train Shape X , Y: {0} , {1} \\t Test Shape X , Y: {2} , {3}\".format(train_x.shape, train_y.shape, test_x.shape, test_y.shape))\n",
    "    \n",
    "\n",
    "    \n",
    "else:      # -- K-fold dataset\n",
    "    print(\"\\n\\n -------------------- K-Fold Dataset ------------------------ \")\n",
    "    folds = []\n",
    "\n",
    "    for i in range(Kfold_num):\n",
    "        train_x, train_y, test_x, test_y = create_dataset(train_Fold[i])\n",
    "        print(\" \\t Train (X,Y): {0} , {1} \\t Test (X,Y): {2} , {3}\".format(train_x.shape, train_y.shape, test_x.shape, test_y.shape))\n",
    "        folds.append((train_x, train_y, test_x, test_y))\n",
    "        \n",
    "    print(np.array(folds[0][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (69590, 15, 3)  , y_train:  (69590, 15, 5)\n",
      "x_test:  (36104, 15, 3)  , y_test:  (36104, 15, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tqdm\n",
    "\n",
    "lookback_window = 15\n",
    "\n",
    "def sliding_window(data_x, data_y, lookback_window=15):\n",
    "    x = []\n",
    "    y = []\n",
    "    enc = MinMaxScaler()\n",
    "    enc_y = enc.fit_transform(data_y)\n",
    "    for i in range(lookback_window, len(data_x)):\n",
    "#     for i in tqdm(range(lookback_window, len(data_x))):\n",
    "        x.append(data_x[i - lookback_window:i])\n",
    "#         y.append(data_y[i])\n",
    "\n",
    "        y.append(data_y[i - lookback_window:i])\n",
    "    return np.array(x), np.array(y), enc\n",
    "\n",
    "x_train, y_train, enc_train = sliding_window(train_x,train_y)\n",
    "x_test, y_test, enc_test = sliding_window(test_x, test_y)\n",
    "print(\"x_train: \", x_train.shape,\" , y_train: \", y_train.shape)\n",
    "print(\"x_test: \",x_test.shape, \" , y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ashishpatel26/tcn-keras-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import compiled_tcn, TCN\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 15, 3)]           0         \n",
      "_________________________________________________________________\n",
      "tcn (TCN)                    (None, 15, 64)            91520     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 15, 5)             325       \n",
      "=================================================================\n",
      "Total params: 91,845\n",
      "Trainable params: 91,845\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_epochs= 7\n",
    "batch_size= 250\n",
    "\n",
    "nb_classes = 5\n",
    "\n",
    "\n",
    "i = Input(shape=(lookback_window, 3))\n",
    "m = TCN(return_sequences=True)(i)\n",
    "m = Dense(nb_classes, activation='softmax')(m)\n",
    "\n",
    "model = Model(inputs=[i], outputs=[m])\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/7\n",
      "279/279 [==============================] - 14s 51ms/step - loss: 0.4708 - accuracy: 0.8343\n",
      "Epoch 2/7\n",
      "279/279 [==============================] - 14s 52ms/step - loss: 0.1920 - accuracy: 0.9215\n",
      "Epoch 3/7\n",
      "279/279 [==============================] - 15s 53ms/step - loss: 0.1766 - accuracy: 0.9260\n",
      "Epoch 4/7\n",
      "279/279 [==============================] - 15s 52ms/step - loss: 0.1719 - accuracy: 0.9270\n",
      "Epoch 5/7\n",
      "279/279 [==============================] - 14s 51ms/step - loss: 0.1676 - accuracy: 0.9286\n",
      "Epoch 6/7\n",
      "279/279 [==============================] - 14s 51ms/step - loss: 0.1635 - accuracy: 0.9298\n",
      "Epoch 7/7\n",
      "279/279 [==============================] - 14s 52ms/step - loss: 0.1610 - accuracy: 0.9314\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "history = model.fit(x_train,  y_train, epochs=nb_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1129/1129 [==============================] - 2s 2ms/step - loss: 0.5940 - accuracy: 0.8423\n",
      "--> Evaluation on Test Dataset:\n",
      "**** Accuracy for Activity Recognition task is:  [0.5939515233039856, 0.8422520160675049]\n"
     ]
    }
   ],
   "source": [
    "# ------------- Evaluation \n",
    "verbosity = 1\n",
    "results_1 = model.evaluate(x_test, y_test, verbose = verbosity)\n",
    "print(\"--> Evaluation on Test Dataset:\")\n",
    "print(\"**** Accuracy for Activity Recognition task is: \", results_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36104, 15, 5)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MinMaxScaler' object has no attribute 'fit_transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7c1df9df514a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_raw_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_raw_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transforms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_raw_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(y_pred.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# incorrect = np.nonzero(y_true_idx != y_pred_idx) # print(incorrect, len(incorrect[0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MinMaxScaler' object has no attribute 'fit_transforms'"
     ]
    }
   ],
   "source": [
    "# ----------------- \n",
    "y_raw_pred = model.predict(x_test)\n",
    "print(y_raw_pred.shape)\n",
    "y_pred = enc_test.inverse_transform(y_raw_pred).flatten()\n",
    "# print(y_pred.shape)\n",
    "# incorrect = np.nonzero(y_true_idx != y_pred_idx) # print(incorrect, len(incorrect[0]))\n",
    "\n",
    "# valid_num = len(correct[0]) + len(incorrect[0])\n",
    "# tot_num = len(y_true_idx)\n",
    "# # print(\"[must same] # of real data: \", tot_num, \"==  # of pred data: \", valid_num)\n",
    "# percent_correct = len(correct[0]) /tot_num\n",
    "# percent_incorrect = len(incorrect[0])/tot_num\n",
    "# print(\" correct: \", percent_correct,\" [%]\", \",  incorrect: \", percent_incorrect,\" [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(history.)\n",
    "# y_vloss = history.history['val_acc']\n",
    "# y_loss = history.history['acc']\n",
    "\n",
    "# x_len = numpy.arange(len(y_loss))\n",
    "# plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n",
    "# plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n",
    "\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.grid()\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the value from x_test\n",
    "y_raw_pred = model.predict(np.array([x_test]))\n",
    "\n",
    "\n",
    "# Invert transform for get a original value\n",
    "y_pred = enc_test.inverse_transform(y_raw_pred).flatten()\n",
    "y_true = enc_test.inverse_transform([y_test]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
